{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5932fcd-b3b7-42b6-ae36-ab77a102b7aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1d6a883-681b-4fd7-9db6-2304d994c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if GROQ_API_KEY is None:\n",
    "    raise ValueError(\"‚ùå Missing GROQ_API_KEY in environment variables.\")\n",
    "\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "def groq_generate(prompt, model):\n",
    "    \"\"\"Generic Groq requester with custom model.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a tweet classifier that detects expressions of hope.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.1,\n",
    "        max_completion_tokens=80,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9937230b-e73f-4962-9482-7955c73e919d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 8256\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>binary</th>\n",
       "      <th>multiclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#USER# #USER# You can‚Äôt solve Nigerias problem...</td>\n",
       "      <td>Not Hope</td>\n",
       "      <td>Not Hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#USER# #USER# #USER# So looking forward to to ...</td>\n",
       "      <td>Hope</td>\n",
       "      <td>Generalized Hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#USER# This is the most runs they‚Äôve scored al...</td>\n",
       "      <td>Not Hope</td>\n",
       "      <td>Not Hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#USER# My point is: any vaccine batches that c...</td>\n",
       "      <td>Not Hope</td>\n",
       "      <td>Not Hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Liberals were happy killing the businesses tha...</td>\n",
       "      <td>Not Hope</td>\n",
       "      <td>Not Hope</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    binary  \\\n",
       "0  #USER# #USER# You can‚Äôt solve Nigerias problem...  Not Hope   \n",
       "1  #USER# #USER# #USER# So looking forward to to ...      Hope   \n",
       "2  #USER# This is the most runs they‚Äôve scored al...  Not Hope   \n",
       "3  #USER# My point is: any vaccine batches that c...  Not Hope   \n",
       "4  Liberals were happy killing the businesses tha...  Not Hope   \n",
       "\n",
       "         multiclass  \n",
       "0          Not Hope  \n",
       "1  Generalized Hope  \n",
       "2          Not Hope  \n",
       "3          Not Hope  \n",
       "4          Not Hope  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"HSD.csv\")\n",
    "print(\"Dataset loaded:\", len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6efce8a-e7b6-46a9-955c-8eae9bada432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary subset: 1000\n",
      "Multiclass subset: 1002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Theas\\AppData\\Local\\Temp\\ipykernel_36524\\865934801.py:9: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(500, random_state=42))\n",
      "C:\\Users\\Theas\\AppData\\Local\\Temp\\ipykernel_36524\\865934801.py:26: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(334, random_state=42))\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Subset BINARIO (Hope vs Not Hope)\n",
    "# ============================\n",
    "subset = df[df['binary'].isin(['Not Hope', 'Hope'])]\n",
    "\n",
    "subsetBin = (\n",
    "    subset\n",
    "    .groupby('binary', group_keys=False)\n",
    "    .apply(lambda x: x.sample(500, random_state=42))\n",
    "    .sample(frac=1, random_state=42)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# Subset MULTICLASS\n",
    "# ============================\n",
    "subset2 = df[df['multiclass'].isin([\n",
    "    'Generalized Hope',\n",
    "    'Realistic Hope',\n",
    "    'Unrealistic Hope'\n",
    "])]\n",
    "\n",
    "subsetMulti = (\n",
    "    subset2\n",
    "    .groupby('multiclass', group_keys=False)\n",
    "    .apply(lambda x: x.sample(334, random_state=42))\n",
    "    .sample(frac=1, random_state=42)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"Binary subset:\", len(subsetBin))\n",
    "print(\"Multiclass subset:\", len(subsetMulti))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0fd2898-08bc-46d2-8085-447a13deee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificarBin_batch(lista_textos, prompt_version=1, df=None, model=None):\n",
    "    \"\"\"\n",
    "    Clasifica una LISTA de textos en:\n",
    "    - Hope\n",
    "    - Not Hope\n",
    "\n",
    "    prompt_version:\n",
    "    1 ‚Üí Zero-shot\n",
    "    2 ‚Üí One-shot (1 ejemplo)\n",
    "    3 ‚Üí Few-shot (hasta 5 ejemplos)\n",
    "\n",
    "    Retorna SIEMPRE len(lista_textos) etiquetas.\n",
    "    \"\"\"\n",
    "\n",
    "    VALID_LABELS = [\"Hope\", \"Not Hope\"]\n",
    "\n",
    "    definicion_hope = \"Hope means expressing optimism, trust, or desire for a better future.\"\n",
    "\n",
    "    # ===================== BLOQUE NUMERADO =====================\n",
    "    bloque_tweets = \"\\n\".join(\n",
    "        f\"{i+1}. {texto}\" for i, texto in enumerate(lista_textos)\n",
    "    )\n",
    "\n",
    "    # ===================== EJEMPLOS =====================\n",
    "    ejemplo_texto = \"\"\n",
    "    ejemplos_texto = \"\"\n",
    "\n",
    "    if df is not None and \"binary\" in df.columns and not df.empty:\n",
    "\n",
    "        # üîπ ONE-SHOT ‚Üí 1 ejemplo\n",
    "        if prompt_version == 2:\n",
    "            row = df.sample(1).iloc[0]\n",
    "            ejemplo_texto = f'\"{row[\"text\"]}\" ‚Üí {row[\"binary\"]}'\n",
    "\n",
    "        # üîπ FEW-SHOT ‚Üí hasta 5 ejemplos\n",
    "        elif prompt_version == 3:\n",
    "            ejemplos = df.sample(min(5, len(df)))\n",
    "            ejemplos_texto = \"\\n\".join(\n",
    "                f'\"{row[\"text\"]}\" ‚Üí {row[\"binary\"]}'\n",
    "                for _, row in ejemplos.iterrows()\n",
    "            )\n",
    "\n",
    "    # Fallback si no hay df\n",
    "    if prompt_version == 2 and ejemplo_texto == \"\":\n",
    "        ejemplo_texto = '\"Things will get better soon.\" ‚Üí Hope'\n",
    "\n",
    "    if prompt_version == 3 and ejemplos_texto == \"\":\n",
    "        ejemplos_texto = '\"Things will get better soon.\" ‚Üí Hope'\n",
    "\n",
    "    # ===================== PROMPTS =====================\n",
    "\n",
    "    # ---------- ZERO SHOT ----------\n",
    "    if prompt_version == 1:\n",
    "        prompt = f\"\"\"\n",
    "Definition: {definicion_hope}\n",
    "\n",
    "You will receive a numbered list of tweets.\n",
    "Return EXACTLY ONE LABEL PER LINE, in the same order.\n",
    "Valid labels:\n",
    "- Hope\n",
    "- Not Hope\n",
    "\n",
    "Tweets:\n",
    "{bloque_tweets}\n",
    "\n",
    "Now output ONLY the labels, one per line, with no numbers and no extra text:\n",
    "\"\"\"\n",
    "\n",
    "    # ---------- ONE SHOT (1 ejemplo) ----------\n",
    "    elif prompt_version == 2:\n",
    "        prompt = f\"\"\"\n",
    "Definition: {definicion_hope}\n",
    "\n",
    "Example:\n",
    "{ejemplo_texto}\n",
    "\n",
    "You will receive a numbered list of tweets.\n",
    "Return EXACTLY ONE LABEL PER LINE, in the same order.\n",
    "Valid labels:\n",
    "- Hope\n",
    "- Not Hope\n",
    "\n",
    "Tweets:\n",
    "{bloque_tweets}\n",
    "\n",
    "Now output ONLY the labels, one per line, with no numbers and no extra text:\n",
    "\"\"\"\n",
    "\n",
    "    # ---------- FEW SHOT (5 ejemplos) ----------\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "Definition: {definicion_hope}\n",
    "\n",
    "Examples:\n",
    "{ejemplos_texto}\n",
    "\n",
    "You will receive a numbered list of tweets.\n",
    "Return EXACTLY ONE LABEL PER LINE, in the same order.\n",
    "Valid labels:\n",
    "- Hope\n",
    "- Not Hope\n",
    "\n",
    "Tweets:\n",
    "{bloque_tweets}\n",
    "\n",
    "Now output ONLY the labels, one per line, with no numbers and no extra text:\n",
    "\"\"\"\n",
    "\n",
    "    # ===================== CALL MODEL =====================\n",
    "    raw = groq_generate(prompt, model=model)\n",
    "    raw_lines = [l.strip() for l in raw.splitlines() if l.strip()]\n",
    "\n",
    "    # ===================== PARSER ROBUSTO =====================\n",
    "    labels = []\n",
    "\n",
    "    for line in raw_lines:\n",
    "        clean = line.strip()\n",
    "\n",
    "        if clean == \"\":\n",
    "            continue\n",
    "        if clean.lower().startswith((\"tweet\", \"label\", \"output\")):\n",
    "            continue\n",
    "        if \":\" in clean:\n",
    "            clean = clean.split(\":\")[-1].strip()\n",
    "\n",
    "        for v in VALID_LABELS:\n",
    "            if v.lower() == clean.lower():\n",
    "                labels.append(v)\n",
    "                break\n",
    "\n",
    "    # ===================== FALLBACK SEGURO =====================\n",
    "    if len(labels) < len(lista_textos):\n",
    "        labels += [\"Not Hope\"] * (len(lista_textos) - len(labels))\n",
    "    elif len(labels) > len(lista_textos):\n",
    "        labels = labels[:len(lista_textos)]\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4fe8fc2-a076-439f-9075-309fabbf4279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificarMulti_batch(lista_textos, prompt_version=1, df=None, model=None):\n",
    "    \"\"\"\n",
    "    Classifies texts using PROMPT LABELS ONLY:\n",
    "    - Generalized Hope\n",
    "    - Realistic Hope\n",
    "    - Fantasy Wish\n",
    "\n",
    "    ‚ùó No dataset labels appear here.\n",
    "    \"\"\"\n",
    "\n",
    "    PROMPT_LABELS = [\n",
    "        \"Generalized Hope\",\n",
    "        \"Realistic Hope\",\n",
    "        \"Fantasy Wish\"\n",
    "    ]\n",
    "\n",
    "    # ---------- Dataset for shots (mapped) ----------\n",
    "    df_hope = None\n",
    "    if df is not None and \"multiclass\" in df.columns:\n",
    "        df_hope = df.copy()\n",
    "        df_hope[\"prompt_label\"] = df_hope[\"multiclass\"].map({\n",
    "            \"Generalized Hope\": \"Generalized Hope\",\n",
    "            \"Abstract Hope\": \"Generalized Hope\",   # replaced Abstract with Generalized\n",
    "            \"Realistic Hope\": \"Realistic Hope\",\n",
    "            \"Unrealistic Hope\": \"Fantasy Wish\"\n",
    "        })\n",
    "        df_hope = df_hope[df_hope[\"prompt_label\"].notna()]\n",
    "\n",
    "    bloque_tweets = \"\\n\".join(f\"{i+1}. {t}\" for i, t in enumerate(lista_textos))\n",
    "\n",
    "    RULES = \"\"\"\n",
    "CLASSIFICATION RULES:\n",
    "\n",
    "FANTASY WISH:\n",
    "- Impossible, miraculous, magical, or unrealistic outcomes.\n",
    "- Fictional, cosmic, or hyperbolic wishes.\n",
    "\n",
    "REALISTIC HOPE:\n",
    "- Hope clearly tied to a specific, real-world situation or event.\n",
    "- Clearly tied to a concrete context (event, date, person, institution).\n",
    "- Plausible within normal reality.\n",
    "\n",
    "GENERALIZED HOPE:\n",
    "- Broad, non-specific expressions of hope.\n",
    "- Blessings, prayers, encouragement, or vague optimism.\n",
    "- NOT clearly tied to a specific event or situation.\n",
    "\"\"\"\n",
    "\n",
    "    # ---------- PROMPTS ----------\n",
    "    if prompt_version == 1:  # Zero-shot\n",
    "        prompt = f\"\"\"\n",
    "Classify each tweet into ONE of:\n",
    "- Generalized Hope\n",
    "- Realistic Hope\n",
    "- Fantasy Wish\n",
    "\n",
    "{RULES}\n",
    "\n",
    "Tweets:\n",
    "{bloque_tweets}\n",
    "\n",
    "Output ONLY one label per line:\n",
    "\"\"\"\n",
    "\n",
    "    elif prompt_version == 2:  # One-shot\n",
    "        ex = df_hope.sample(1).iloc[0] if df_hope is not None else None\n",
    "        ej_text = ex[\"text\"] if ex is not None else \"I hope a miracle fixes everything overnight.\"\n",
    "        ej_label = ex[\"prompt_label\"] if ex is not None else \"Fantasy Wish\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Classify each tweet into ONE of:\n",
    "- Generalized Hope\n",
    "- Realistic Hope\n",
    "- Fantasy Wish\n",
    "\n",
    "{RULES}\n",
    "\n",
    "Example:\n",
    "\"{ej_text}\" ‚Üí {ej_label}\n",
    "\n",
    "Tweets:\n",
    "{bloque_tweets}\n",
    "\n",
    "Output ONLY one label per line:\n",
    "\"\"\"\n",
    "\n",
    "    else:  # Few-shot\n",
    "        ejemplos = []\n",
    "        if df_hope is not None:\n",
    "            for lbl in PROMPT_LABELS:\n",
    "                sub = df_hope[df_hope[\"prompt_label\"] == lbl]\n",
    "                if not sub.empty:\n",
    "                    r = sub.sample(1).iloc[0]\n",
    "                    ejemplos.append(f'\"{r[\"text\"]}\" ‚Üí {lbl}')\n",
    "\n",
    "        ejemplos_texto = \"\\n\".join(ejemplos)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Classify each tweet into ONE of:\n",
    "- Generalized Hope\n",
    "- Realistic Hope\n",
    "- Fantasy Wish\n",
    "\n",
    "{RULES}\n",
    "\n",
    "Examples:\n",
    "{ejemplos_texto}\n",
    "\n",
    "Tweets:\n",
    "{bloque_tweets}\n",
    "\n",
    "Output ONLY one label per line:\n",
    "\"\"\"\n",
    "\n",
    "    raw = groq_generate(prompt, model=model)\n",
    "    raw_lines = [l.strip() for l in raw.splitlines() if l.strip()]\n",
    "\n",
    "    def normalize(label):\n",
    "        t = label.lower()\n",
    "        if \"fantasy\" in t:\n",
    "            return \"Fantasy Wish\"\n",
    "        if \"realistic\" in t:\n",
    "            return \"Realistic Hope\"\n",
    "        if \"generalized\" in t or \"abstract\" in t:\n",
    "            return \"Generalized Hope\"  # unify Abstract ‚Üí Generalized\n",
    "        # fallback\n",
    "        return \"Generalized Hope\"\n",
    "\n",
    "    labels = [normalize(l) for l in raw_lines]\n",
    "\n",
    "    if len(labels) < len(lista_textos):\n",
    "        labels += [\"Generalized Hope\"] * (len(lista_textos) - len(labels))\n",
    "    elif len(labels) > len(lista_textos):\n",
    "        labels = labels[:len(lista_textos)]  # <-- fixed\n",
    "\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fd89ced-629d-4e1f-a5ab-15d68eb24ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    MODELS = {\n",
    "        # ---------- LLaMA family ----------\n",
    "        \"LLaMA-8B\": \"llama-3.1-8b-instant\",\n",
    "        \"LLaMA-70B\": \"llama-3.3-70b-versatile\",\n",
    "    \n",
    "        # ---------- LLaMA-4 (Instruct variants) ----------\n",
    "        \"Maverick-17B\": \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "        \"Scout-17B\": \"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    \n",
    "        # ---------- External / Non-LLaMA ----------\n",
    "        \"Allam-7B\": \"allam-2-7b\",\n",
    "        \"Kimi-K2\": \"moonshotai/kimi-k2-instruct\",\n",
    "        \"Qwen-32B\": \"qwen/qwen3-32b\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d8976-694c-4be4-8e1d-20043da17eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from groq import RateLimitError\n",
    "import pandas as pd\n",
    "\n",
    "RESULTS = []\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "NUM_RECORDS = 200\n",
    "\n",
    "finBin = subsetBin.sample(NUM_RECORDS).copy()  # ‚úÖ SOLO binary\n",
    "\n",
    "# ‚õî MODELOS A SALTAR\n",
    "SKIP_MODELS = []\n",
    "\n",
    "print(\"‚è≠Ô∏è Skipping models:\", SKIP_MODELS)\n",
    "\n",
    "# ===================== LOOP PRINCIPAL =====================\n",
    "for model_name, model_id in tqdm(\n",
    "    MODELS.items(),\n",
    "    desc=\"Modelos (Binary)\",\n",
    "    total=len(MODELS)\n",
    "):\n",
    "\n",
    "    if model_name in SKIP_MODELS:\n",
    "        print(f\"\\n‚è≠Ô∏è SKIPPING MODEL (hardcoded): {model_name}\\n\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n==========================\")\n",
    "    print(f\"Testing MODEL (Binary): {model_name}\")\n",
    "    print(\"==========================\\n\")\n",
    "\n",
    "    try:\n",
    "        for prompt_version in tqdm(\n",
    "            [1, 2, 3],\n",
    "            desc=f\"Prompts ({model_name})\"\n",
    "        ):\n",
    "\n",
    "            col = f\"{model_name}_p{prompt_version}\"\n",
    "            if col in finBin.columns:\n",
    "                print(f\"‚è≠Ô∏è {col} already exists, skipping\")\n",
    "                continue\n",
    "\n",
    "            preds_bin = []\n",
    "            batches_bin = range(0, len(finBin), BATCH_SIZE)\n",
    "\n",
    "            for i in tqdm(\n",
    "                batches_bin,\n",
    "                desc=f\"Binary p{prompt_version} ‚Äî {model_name}\",\n",
    "                leave=False\n",
    "            ):\n",
    "                batch = finBin[\"text\"].iloc[i:i+BATCH_SIZE].tolist()\n",
    "                preds_bin.extend(\n",
    "                    clasificarBin_batch(\n",
    "                        batch,\n",
    "                        prompt_version,\n",
    "                        df=subsetBin,\n",
    "                        model=model_id\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            finBin[col] = preds_bin\n",
    "            print(f\"{model_name} ‚Äî Binary Prompt {prompt_version} DONE.\")\n",
    "\n",
    "        # üîπ Guardado parcial por modelo\n",
    "        finBin.to_csv(f\"resultados_binary_{model_name}.csv\", index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Error en modelo {model_name}: {e}\")\n",
    "        print(\"‚è≠Ô∏è Saltando este modelo y continuando.\\n\")\n",
    "        continue\n",
    "\n",
    "# ===================== GUARDADO FINAL =====================\n",
    "finBin.to_csv(\"Btempraw, index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Evaluaci√≥n BINARIA terminada. CSV guardado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e55bedf-da29-4ff4-8b59-cb6924e8ea3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≠Ô∏è Skipping models: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b60ac223a0747d5ae31966b09ac4353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Modelos:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      "Testing MODEL: LLaMA-8B\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d81974410fe84df587d24fd4a350615c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prompts (LLaMA-8B):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598f297c1a28411b91669da63bc7b8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p1 ‚Äî LLaMA-8B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-8B ‚Äî Prompt 1 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccb2970d5ff402ea2a71330a5ac7ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p2 ‚Äî LLaMA-8B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-8B ‚Äî Prompt 2 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb5f52db46541fda98db6cc0f4e6136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p3 ‚Äî LLaMA-8B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-8B ‚Äî Prompt 3 DONE.\n",
      "\n",
      "==========================\n",
      "Testing MODEL: LLaMA-70B\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1611167386d740e6a5110e79f7137e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prompts (LLaMA-70B):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f02fcbb4685245cdbb7ecb1312b6e606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p1 ‚Äî LLaMA-70B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-70B ‚Äî Prompt 1 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521b00bbc802462d8dc9a46b12f78368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p2 ‚Äî LLaMA-70B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-70B ‚Äî Prompt 2 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4350a37b9cf447cbac8c3046124d1269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p3 ‚Äî LLaMA-70B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-70B ‚Äî Prompt 3 DONE.\n",
      "\n",
      "==========================\n",
      "Testing MODEL: Maverick-17B\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d1841478284287b9b56805f79625fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prompts (Maverick-17B):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fde3bd85bf4f03b34a5985b1179024",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p1 ‚Äî Maverick-17B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maverick-17B ‚Äî Prompt 1 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3ebe86ebcb495cb4022654dbefebc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p2 ‚Äî Maverick-17B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maverick-17B ‚Äî Prompt 2 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62855ee4e6b42c28766f96a6dd9f432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p3 ‚Äî Maverick-17B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maverick-17B ‚Äî Prompt 3 DONE.\n",
      "\n",
      "==========================\n",
      "Testing MODEL: Scout-17B\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1f4af110a7476faa23c12ed7f08c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prompts (Scout-17B):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5816e9558b446b48fea1078c8039171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p1 ‚Äî Scout-17B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scout-17B ‚Äî Prompt 1 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f860b691f6844f7a829f11cbf8a67dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p2 ‚Äî Scout-17B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scout-17B ‚Äî Prompt 2 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1745a210da664c9fbb9c2a802b6bb78a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p3 ‚Äî Scout-17B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allam-7B ‚Äî Prompt 2 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1cb4224f004ab4bdaec2363d4bf986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p3 ‚Äî Allam-7B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allam-7B ‚Äî Prompt 3 DONE.\n",
      "\n",
      "==========================\n",
      "Testing MODEL: Kimi-K2\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ac1d2bf7c442e294b8bd965a77a52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prompts (Kimi-K2):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "564f0a052b4441bd980f711a97a53087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p1 ‚Äî Kimi-K2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kimi-K2 ‚Äî Prompt 1 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd69306f7f949efac0077b1e3ab0639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p2 ‚Äî Kimi-K2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kimi-K2 ‚Äî Prompt 2 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f441784f98a4fdd8fc10eef2923df15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p3 ‚Äî Kimi-K2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kimi-K2 ‚Äî Prompt 3 DONE.\n",
      "\n",
      "==========================\n",
      "Testing MODEL: Qwen-32B\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c72d6c5fac4e7b8a3a16c421a8fd8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prompts (Qwen-32B):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56fb247609754ef7b2681356c3b96029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p1 ‚Äî Qwen-32B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen-32B ‚Äî Prompt 1 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b11022c5884daf9048d2faac02a4b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p2 ‚Äî Qwen-32B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen-32B ‚Äî Prompt 2 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a9bff0a4a54e9b98654d9988f75fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p3 ‚Äî Qwen-32B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen-32B ‚Äî Prompt 3 DONE.\n",
      "\n",
      "‚úÖ Evaluaci√≥n terminada. CSV guardado correctamente.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from groq import RateLimitError\n",
    "import pandas as pd\n",
    "\n",
    "RESULTS = []\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "NUM_RECORDS = 200\n",
    "\n",
    "# Sample a subset of the dataset\n",
    "finMulti = subsetMulti.sample(NUM_RECORDS).copy()\n",
    "\n",
    "SKIP_MODELS = []\n",
    "print(\"‚è≠Ô∏è Skipping models:\", SKIP_MODELS)\n",
    "\n",
    "for model_name, model_id in tqdm(MODELS.items(), desc=\"Modelos\", total=len(MODELS)):\n",
    "\n",
    "    if model_name in SKIP_MODELS:\n",
    "        print(f\"\\n‚è≠Ô∏è SKIPPING MODEL (hardcoded): {model_name}\\n\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n==========================\")\n",
    "    print(f\"Testing MODEL: {model_name}\")\n",
    "    print(\"==========================\\n\")\n",
    "\n",
    "    try:\n",
    "        for prompt_version in tqdm([1, 2, 3], desc=f\"Prompts ({model_name})\"):\n",
    "\n",
    "            col = f\"{model_name}_p{prompt_version}\"\n",
    "            if col in finMulti.columns:\n",
    "                print(f\"‚è≠Ô∏è {col} already exists, skipping\")\n",
    "                continue\n",
    "\n",
    "            preds_multi = []\n",
    "            batches_multi = range(0, len(finMulti), BATCH_SIZE)\n",
    "\n",
    "            for i in tqdm(\n",
    "                batches_multi,\n",
    "                desc=f\"Multi p{prompt_version} ‚Äî {model_name}\",\n",
    "                leave=False\n",
    "            ):\n",
    "                batch = finMulti[\"text\"].iloc[i:i+BATCH_SIZE].tolist()\n",
    "                batch_preds = clasificarMulti_batch(\n",
    "                    batch,\n",
    "                    prompt_version,\n",
    "                    df=subsetMulti,\n",
    "                    model=model_id\n",
    "                )\n",
    "\n",
    "                # Sanity check: all labels are valid\n",
    "                for lbl in batch_preds:\n",
    "                    if lbl not in [\"Generalized Hope\", \"Realistic Hope\", \"Fantasy Wish\"]:\n",
    "                        print(f\"‚ö†Ô∏è Unexpected label: {lbl}\")\n",
    "                        lbl = \"Generalized Hope\"\n",
    "\n",
    "                preds_multi.extend(batch_preds)\n",
    "\n",
    "            finMulti[col] = preds_multi\n",
    "            print(f\"{model_name} ‚Äî Prompt {prompt_version} DONE.\")\n",
    "\n",
    "        # Save per model\n",
    "        finMulti.to_csv(f\"resultados_multiclass_{model_name}.csv\", index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Error en modelo {model_name}: {e}\")\n",
    "        print(\"‚è≠Ô∏è Saltando este modelo y continuando.\\n\")\n",
    "        continue\n",
    "\n",
    "# Save final aggregated CSV\n",
    "finMulti.to_csv(\"Mtempraw.csv\", index=False)\n",
    "print(\"\\n‚úÖ Evaluaci√≥n terminada. CSV guardado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f745065-71ee-42a1-98b3-639a4d6aae34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"===== üß† BINARY CLASSIFICATION (ALL MODELS) =====\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "def normalize_binary(label):\n",
    "    if not isinstance(label, str):\n",
    "        return \"Not Hope\"\n",
    "    t = label.strip().lower()\n",
    "    if \"not hope\" in t:\n",
    "        return \"Not Hope\"\n",
    "    if t == \"hope\":\n",
    "        return \"Hope\"\n",
    "    if \"hope\" in t and \"not\" not in t:\n",
    "        return \"Hope\"\n",
    "    return \"Not Hope\"\n",
    "\n",
    "os.makedirs(\"used_prompts\", exist_ok=True)\n",
    "\n",
    "for model_name in MODELS.keys():\n",
    "    print(f\"\\n======================\")\n",
    "    print(f\"MODEL: {model_name}\")\n",
    "    print(\"======================\")\n",
    "\n",
    "    for p in [1, 2, 3]:\n",
    "        col = f\"{model_name}_p{p}\"\n",
    "\n",
    "        if col not in finBin.columns:\n",
    "            print(f\"‚è≠Ô∏è Missing {col}, skipping\")\n",
    "            continue\n",
    "\n",
    "        finBin[f\"{col}_norm\"] = finBin[col].apply(normalize_binary)\n",
    "\n",
    "        y_true = finBin[\"binary\"]\n",
    "        y_pred = finBin[f\"{col}_norm\"]\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        report = classification_report(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            output_dict=True,\n",
    "            zero_division=0\n",
    "        )\n",
    "\n",
    "        macro_f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "        invalid_outputs = sum(\n",
    "            1 for x in finBin[col]\n",
    "            if not isinstance(x, str)\n",
    "            or (\"hope\" not in x.lower())\n",
    "        )\n",
    "\n",
    "        print(f\"\\n--- Prompt {p} ---\")\n",
    "        print(f\"Accuracy: {acc*100:.2f}%\")\n",
    "        print(f\"Macro F1 (PRIMARY): {macro_f1:.4f}\")\n",
    "        print(classification_report(y_true, y_pred, zero_division=0))\n",
    "\n",
    "        for label in [\"Hope\", \"Not Hope\"]:\n",
    "            rows.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Prompt\": p,\n",
    "                \"PromptType\": [\"zero-shot\", \"one-shot\", \"few-shot\"][p-1],\n",
    "                \"Label\": label,\n",
    "                \"Precision\": report[label][\"precision\"],\n",
    "                \"Recall\": report[label][\"recall\"],\n",
    "                \"F1\": report[label][\"f1-score\"],\n",
    "                \"Support\": report[label][\"support\"],\n",
    "                \"Accuracy\": acc,\n",
    "                \"Primary_Macro_F1\": macro_f1,\n",
    "                \"InvalidOutputs\": invalid_outputs\n",
    "            })\n",
    "\n",
    "        rows.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Prompt\": p,\n",
    "            \"PromptType\": [\"zero-shot\", \"one-shot\", \"few-shot\"][p-1],\n",
    "            \"Label\": \"macro avg\",\n",
    "            \"Precision\": report[\"macro avg\"][\"precision\"],\n",
    "            \"Recall\": report[\"macro avg\"][\"recall\"],\n",
    "            \"F1\": macro_f1,\n",
    "            \"Support\": report[\"macro avg\"][\"support\"],\n",
    "            \"Accuracy\": acc,\n",
    "            \"Primary_Macro_F1\": macro_f1,\n",
    "            \"InvalidOutputs\": invalid_outputs\n",
    "        })\n",
    "\n",
    "        rows.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Prompt\": p,\n",
    "            \"PromptType\": [\"zero-shot\", \"one-shot\", \"few-shot\"][p-1],\n",
    "            \"Label\": \"weighted avg\",\n",
    "            \"Precision\": report[\"weighted avg\"][\"precision\"],\n",
    "            \"Recall\": report[\"weighted avg\"][\"recall\"],\n",
    "            \"F1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "            \"Support\": report[\"weighted avg\"][\"support\"],\n",
    "            \"Accuracy\": acc,\n",
    "            \"Primary_Macro_F1\": macro_f1,\n",
    "            \"InvalidOutputs\": invalid_outputs\n",
    "        })\n",
    "\n",
    "        with open(f\"used_prompts/binary_{model_name}_prompt{p}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"MODEL: {model_name}\\nPROMPT VERSION: {p}\\n\")\n",
    "\n",
    "df_binary_metrics = pd.DataFrame(rows)\n",
    "df_binary_metrics.to_csv(\"Btemp.csv\", index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Saved: Btemp.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c23c5f7c-4c71-4a38-955a-c25e66441873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== üé® MULTICLASS CLASSIFICATION (ALL MODELS) =====\n",
      "\n",
      "======================\n",
      "MODEL: LLaMA-8B\n",
      "======================\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Accuracy: 49.50%\n",
      "Macro F1 (PRIMARY): 0.4796\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.43      0.73      0.54        75\n",
      "  Realistic Hope       0.65      0.32      0.43        68\n",
      "Unrealistic Hope       0.59      0.39      0.47        57\n",
      "\n",
      "        accuracy                           0.49       200\n",
      "       macro avg       0.56      0.48      0.48       200\n",
      "    weighted avg       0.55      0.49      0.48       200\n",
      "\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Accuracy: 47.00%\n",
      "Macro F1 (PRIMARY): 0.4561\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.40      0.69      0.51        75\n",
      "  Realistic Hope       0.62      0.31      0.41        68\n",
      "Unrealistic Hope       0.57      0.37      0.45        57\n",
      "\n",
      "        accuracy                           0.47       200\n",
      "       macro avg       0.53      0.46      0.46       200\n",
      "    weighted avg       0.52      0.47      0.46       200\n",
      "\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Accuracy: 47.00%\n",
      "Macro F1 (PRIMARY): 0.4541\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.41      0.69      0.52        75\n",
      "  Realistic Hope       0.62      0.31      0.41        68\n",
      "Unrealistic Hope       0.53      0.37      0.43        57\n",
      "\n",
      "        accuracy                           0.47       200\n",
      "       macro avg       0.52      0.46      0.45       200\n",
      "    weighted avg       0.51      0.47      0.46       200\n",
      "\n",
      "\n",
      "======================\n",
      "MODEL: LLaMA-70B\n",
      "======================\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Accuracy: 45.00%\n",
      "Macro F1 (PRIMARY): 0.4171\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.41      0.63      0.50        75\n",
      "  Realistic Hope       0.45      0.49      0.47        68\n",
      "Unrealistic Hope       0.77      0.18      0.29        57\n",
      "\n",
      "        accuracy                           0.45       200\n",
      "       macro avg       0.54      0.43      0.42       200\n",
      "    weighted avg       0.53      0.45      0.43       200\n",
      "\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Accuracy: 44.50%\n",
      "Macro F1 (PRIMARY): 0.4168\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.41      0.63      0.49        75\n",
      "  Realistic Hope       0.45      0.46      0.45        68\n",
      "Unrealistic Hope       0.73      0.19      0.31        57\n",
      "\n",
      "        accuracy                           0.45       200\n",
      "       macro avg       0.53      0.43      0.42       200\n",
      "    weighted avg       0.51      0.45      0.43       200\n",
      "\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Accuracy: 48.00%\n",
      "Macro F1 (PRIMARY): 0.4602\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.44      0.65      0.53        75\n",
      "  Realistic Hope       0.48      0.47      0.47        68\n",
      "Unrealistic Hope       0.68      0.26      0.38        57\n",
      "\n",
      "        accuracy                           0.48       200\n",
      "       macro avg       0.53      0.46      0.46       200\n",
      "    weighted avg       0.52      0.48      0.47       200\n",
      "\n",
      "\n",
      "======================\n",
      "MODEL: Maverick-17B\n",
      "======================\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Accuracy: 53.50%\n",
      "Macro F1 (PRIMARY): 0.5335\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.48      0.68      0.56        75\n",
      "  Realistic Hope       0.53      0.41      0.46        68\n",
      "Unrealistic Hope       0.70      0.49      0.58        57\n",
      "\n",
      "        accuracy                           0.54       200\n",
      "       macro avg       0.57      0.53      0.53       200\n",
      "    weighted avg       0.56      0.54      0.53       200\n",
      "\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Accuracy: 56.50%\n",
      "Macro F1 (PRIMARY): 0.5672\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.49      0.71      0.58        75\n",
      "  Realistic Hope       0.56      0.43      0.48        68\n",
      "Unrealistic Hope       0.78      0.54      0.64        57\n",
      "\n",
      "        accuracy                           0.56       200\n",
      "       macro avg       0.61      0.56      0.57       200\n",
      "    weighted avg       0.59      0.56      0.56       200\n",
      "\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Accuracy: 55.00%\n",
      "Macro F1 (PRIMARY): 0.5530\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.48      0.69      0.57        75\n",
      "  Realistic Hope       0.52      0.41      0.46        68\n",
      "Unrealistic Hope       0.79      0.53      0.63        57\n",
      "\n",
      "        accuracy                           0.55       200\n",
      "       macro avg       0.60      0.54      0.55       200\n",
      "    weighted avg       0.58      0.55      0.55       200\n",
      "\n",
      "\n",
      "======================\n",
      "MODEL: Scout-17B\n",
      "======================\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Accuracy: 33.50%\n",
      "Macro F1 (PRIMARY): 0.3021\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.35      0.57      0.43        75\n",
      "  Realistic Hope       0.34      0.21      0.26        68\n",
      "Unrealistic Hope       0.28      0.18      0.22        57\n",
      "\n",
      "        accuracy                           0.34       200\n",
      "       macro avg       0.32      0.32      0.30       200\n",
      "    weighted avg       0.33      0.34      0.31       200\n",
      "\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Accuracy: 35.50%\n",
      "Macro F1 (PRIMARY): 0.3251\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.35      0.60      0.45        75\n",
      "  Realistic Hope       0.33      0.19      0.24        68\n",
      "Unrealistic Hope       0.39      0.23      0.29        57\n",
      "\n",
      "        accuracy                           0.35       200\n",
      "       macro avg       0.36      0.34      0.33       200\n",
      "    weighted avg       0.36      0.35      0.33       200\n",
      "\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Accuracy: 35.00%\n",
      "Macro F1 (PRIMARY): 0.3267\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.35      0.56      0.43        75\n",
      "  Realistic Hope       0.34      0.21      0.26        68\n",
      "Unrealistic Hope       0.37      0.25      0.29        57\n",
      "\n",
      "        accuracy                           0.35       200\n",
      "       macro avg       0.35      0.34      0.33       200\n",
      "    weighted avg       0.35      0.35      0.33       200\n",
      "\n",
      "\n",
      "======================\n",
      "MODEL: Allam-7B\n",
      "======================\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Accuracy: 34.00%\n",
      "Macro F1 (PRIMARY): 0.3290\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.37      0.47      0.41        75\n",
      "  Realistic Hope       0.30      0.26      0.28        68\n",
      "Unrealistic Hope       0.33      0.26      0.29        57\n",
      "\n",
      "        accuracy                           0.34       200\n",
      "       macro avg       0.33      0.33      0.33       200\n",
      "    weighted avg       0.34      0.34      0.33       200\n",
      "\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Accuracy: 33.00%\n",
      "Macro F1 (PRIMARY): 0.3166\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.35      0.47      0.40        75\n",
      "  Realistic Hope       0.35      0.25      0.29        68\n",
      "Unrealistic Hope       0.27      0.25      0.26        57\n",
      "\n",
      "        accuracy                           0.33       200\n",
      "       macro avg       0.32      0.32      0.32       200\n",
      "    weighted avg       0.33      0.33      0.32       200\n",
      "\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Accuracy: 33.00%\n",
      "Macro F1 (PRIMARY): 0.3196\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.37      0.44      0.40        75\n",
      "  Realistic Hope       0.32      0.21      0.25        68\n",
      "Unrealistic Hope       0.29      0.33      0.31        57\n",
      "\n",
      "        accuracy                           0.33       200\n",
      "       macro avg       0.32      0.33      0.32       200\n",
      "    weighted avg       0.33      0.33      0.32       200\n",
      "\n",
      "\n",
      "======================\n",
      "MODEL: Kimi-K2\n",
      "======================\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Accuracy: 59.00%\n",
      "Macro F1 (PRIMARY): 0.5943\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.55      0.56      0.55        75\n",
      "  Realistic Hope       0.56      0.66      0.60        68\n",
      "Unrealistic Hope       0.74      0.54      0.63        57\n",
      "\n",
      "        accuracy                           0.59       200\n",
      "       macro avg       0.61      0.59      0.59       200\n",
      "    weighted avg       0.60      0.59      0.59       200\n",
      "\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Accuracy: 56.50%\n",
      "Macro F1 (PRIMARY): 0.5654\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.53      0.57      0.55        75\n",
      "  Realistic Hope       0.55      0.62      0.58        68\n",
      "Unrealistic Hope       0.67      0.49      0.57        57\n",
      "\n",
      "        accuracy                           0.56       200\n",
      "       macro avg       0.58      0.56      0.57       200\n",
      "    weighted avg       0.57      0.56      0.56       200\n",
      "\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Accuracy: 57.50%\n",
      "Macro F1 (PRIMARY): 0.5748\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.55      0.53      0.54        75\n",
      "  Realistic Hope       0.54      0.69      0.61        68\n",
      "Unrealistic Hope       0.70      0.49      0.58        57\n",
      "\n",
      "        accuracy                           0.57       200\n",
      "       macro avg       0.60      0.57      0.57       200\n",
      "    weighted avg       0.59      0.57      0.57       200\n",
      "\n",
      "\n",
      "======================\n",
      "MODEL: Qwen-32B\n",
      "======================\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Accuracy: 38.00%\n",
      "Macro F1 (PRIMARY): 0.2283\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.38      0.95      0.54        75\n",
      "  Realistic Hope       0.00      0.00      0.00        68\n",
      "Unrealistic Hope       0.45      0.09      0.15        57\n",
      "\n",
      "        accuracy                           0.38       200\n",
      "       macro avg       0.28      0.34      0.23       200\n",
      "    weighted avg       0.27      0.38      0.24       200\n",
      "\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Accuracy: 37.50%\n",
      "Macro F1 (PRIMARY): 0.2191\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.38      0.95      0.54        75\n",
      "  Realistic Hope       0.00      0.00      0.00        68\n",
      "Unrealistic Hope       0.40      0.07      0.12        57\n",
      "\n",
      "        accuracy                           0.38       200\n",
      "       macro avg       0.26      0.34      0.22       200\n",
      "    weighted avg       0.25      0.38      0.24       200\n",
      "\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Accuracy: 37.00%\n",
      "Macro F1 (PRIMARY): 0.2172\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.37      0.93      0.53        75\n",
      "  Realistic Hope       0.00      0.00      0.00        68\n",
      "Unrealistic Hope       0.40      0.07      0.12        57\n",
      "\n",
      "        accuracy                           0.37       200\n",
      "       macro avg       0.26      0.33      0.22       200\n",
      "    weighted avg       0.25      0.37      0.23       200\n",
      "\n",
      "\n",
      "‚úÖ Saved: Mtemp.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"===== üé® MULTICLASS CLASSIFICATION (ALL MODELS) =====\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "# ============================\n",
    "# Etiquetas v√°lidas finales\n",
    "# ============================\n",
    "VALID_LABELS = [\n",
    "    \"Generalized Hope\",\n",
    "    \"Realistic Hope\",\n",
    "    \"Unrealistic Hope\"\n",
    "]\n",
    "\n",
    "# ============================\n",
    "# Normalizaci√≥n ROBUSTA\n",
    "# ============================\n",
    "def normalize_multiclass(label):\n",
    "    if not isinstance(label, str):\n",
    "        return \"Generalized Hope\"\n",
    "\n",
    "    t = label.lower()\n",
    "    if \"fantasy\" in t:\n",
    "        return \"Unrealistic Hope\"\n",
    "    if \"realistic\" in t:\n",
    "        return \"Realistic Hope\"\n",
    "    if \"abstract\" in t:\n",
    "        return \"Generalized Hope\"\n",
    "\n",
    "    return \"Generalized Hope\"\n",
    "\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Guardar prompts usados\n",
    "# ============================\n",
    "os.makedirs(\"used_prompts\", exist_ok=True)\n",
    "\n",
    "# ============================\n",
    "# LOOP POR MODELO Y PROMPT\n",
    "# ============================\n",
    "for model_name in MODELS.keys():\n",
    "    print(f\"\\n======================\")\n",
    "    print(f\"MODEL: {model_name}\")\n",
    "    print(\"======================\")\n",
    "\n",
    "    for p in [1, 2, 3]:\n",
    "        col = f\"{model_name}_p{p}\"\n",
    "\n",
    "        if col not in finMulti.columns:\n",
    "            print(f\"‚è≠Ô∏è Missing {col}, skipping\")\n",
    "            continue\n",
    "\n",
    "        # Normalizar predicciones\n",
    "        finMulti[f\"{col}_norm\"] = finMulti[col].apply(normalize_multiclass)\n",
    "\n",
    "        y_true = finMulti[\"multiclass\"]\n",
    "        y_pred = finMulti[f\"{col}_norm\"]\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        report = classification_report(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            output_dict=True,\n",
    "            zero_division=0\n",
    "        )\n",
    "\n",
    "        macro_f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "        # Conteo de outputs inv√°lidos (antes de normalizar)\n",
    "        invalid_outputs = sum(\n",
    "            1 for x in finMulti[col]\n",
    "            if not isinstance(x, str)\n",
    "            or all(lbl.lower() not in x.lower() for lbl in [\"general\", \"realistic\", \"fantasy\", \"unrealistic\"])\n",
    "        )\n",
    "\n",
    "        print(f\"\\n--- Prompt {p} ---\")\n",
    "        print(f\"Accuracy: {acc*100:.2f}%\")\n",
    "        print(f\"Macro F1 (PRIMARY): {macro_f1:.4f}\")\n",
    "        print(classification_report(y_true, y_pred, zero_division=0))\n",
    "\n",
    "        # ============================\n",
    "        # M√©tricas por clase\n",
    "        # ============================\n",
    "        for label in VALID_LABELS:\n",
    "            rows.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Prompt\": p,\n",
    "                \"PromptType\": [\"zero-shot\", \"one-shot\", \"few-shot\"][p-1],\n",
    "                \"Label\": label,\n",
    "                \"Precision\": report[label][\"precision\"],\n",
    "                \"Recall\": report[label][\"recall\"],\n",
    "                \"F1\": report[label][\"f1-score\"],\n",
    "                \"Support\": report[label][\"support\"],\n",
    "                \"Accuracy\": acc,\n",
    "                \"Primary_Macro_F1\": macro_f1,\n",
    "                \"InvalidOutputs\": invalid_outputs\n",
    "            })\n",
    "\n",
    "        # Macro avg\n",
    "        rows.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Prompt\": p,\n",
    "            \"PromptType\": [\"zero-shot\", \"one-shot\", \"few-shot\"][p-1],\n",
    "            \"Label\": \"macro avg\",\n",
    "            \"Precision\": report[\"macro avg\"][\"precision\"],\n",
    "            \"Recall\": report[\"macro avg\"][\"recall\"],\n",
    "            \"F1\": macro_f1,\n",
    "            \"Support\": report[\"macro avg\"][\"support\"],\n",
    "            \"Accuracy\": acc,\n",
    "            \"Primary_Macro_F1\": macro_f1,\n",
    "            \"InvalidOutputs\": invalid_outputs\n",
    "        })\n",
    "\n",
    "        # Weighted avg\n",
    "        rows.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Prompt\": p,\n",
    "            \"PromptType\": [\"zero-shot\", \"one-shot\", \"few-shot\"][p-1],\n",
    "            \"Label\": \"weighted avg\",\n",
    "            \"Precision\": report[\"weighted avg\"][\"precision\"],\n",
    "            \"Recall\": report[\"weighted avg\"][\"recall\"],\n",
    "            \"F1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "            \"Support\": report[\"weighted avg\"][\"support\"],\n",
    "            \"Accuracy\": acc,\n",
    "            \"Primary_Macro_F1\": macro_f1,\n",
    "            \"InvalidOutputs\": invalid_outputs\n",
    "        })\n",
    "\n",
    "        # Guardar prompt usado (referencia metodol√≥gica)\n",
    "        with open(f\"used_prompts/multiclass_{model_name}_prompt{p}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"MODEL: {model_name}\\nPROMPT VERSION: {p}\\n\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Guardar CSV final\n",
    "# =========================\n",
    "df_multi_metrics = pd.DataFrame(rows)\n",
    "df_multi_metrics.to_csv(\n",
    "    \"Mtemp.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Saved: Mtemp.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358cc40c-a514-4a99-849b-b7982ac50b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.makedirs(\"confusion_matrices\", exist_ok=True)\n",
    "\n",
    "for model_name in MODELS.keys():\n",
    "    for p in [1, 2, 3]:\n",
    "        col = f\"{model_name}_p{p}\"\n",
    "        if col not in finMulti.columns:\n",
    "            continue\n",
    "\n",
    "        y_true = finMulti[\"multiclass\"]\n",
    "        y_pred = finMulti[col].apply(normalize_multiclass)\n",
    "\n",
    "        labels = [\n",
    "            \"Generalized Hope\",\n",
    "            \"Realistic Hope\",\n",
    "            \"Unrealistic Hope\"\n",
    "        ]\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "        disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "        plt.title(f\"{model_name} ‚Äî Prompt {p}\")\n",
    "        plt.savefig(f\"confusion_matrices/{model_name}_prompt{p}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd16c31-8928-4c22-8a09-8ed3239d2f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
