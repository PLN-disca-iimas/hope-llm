{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5932fcd-b3b7-42b6-ae36-ab77a102b7aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from groq import Groq\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1d6a883-681b-4fd7-9db6-2304d994c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "if GROQ_API_KEY is None:\n",
    "    raise ValueError(\"‚ùå Missing GROQ_API_KEY in environment variables.\")\n",
    "\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "def groq_generate(prompt, model):\n",
    "    \"\"\"Generic Groq requester with custom model.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a tweet classifier that detects expressions of hope.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.1,\n",
    "        max_completion_tokens=80,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9937230b-e73f-4962-9482-7955c73e919d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 19183\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>binary</th>\n",
       "      <th>multiclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>todo amor que yo esper√© de la vida lo he encon...</td>\n",
       "      <td>Hope</td>\n",
       "      <td>Generalized Hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hola #USER#   cuando van poner cajas autoservi...</td>\n",
       "      <td>Not Hope</td>\n",
       "      <td>Not Hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#USER# Se√±or Mateu pero este tipo de imagen se...</td>\n",
       "      <td>Not Hope</td>\n",
       "      <td>Not Hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#EspnF90 el var se cre√≥ para ayudar que los √°r...</td>\n",
       "      <td>Not Hope</td>\n",
       "      <td>Not Hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hay un personaje de la primera peli que me rec...</td>\n",
       "      <td>Not Hope</td>\n",
       "      <td>Not Hope</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    binary  \\\n",
       "0  todo amor que yo esper√© de la vida lo he encon...      Hope   \n",
       "1  Hola #USER#   cuando van poner cajas autoservi...  Not Hope   \n",
       "2  #USER# Se√±or Mateu pero este tipo de imagen se...  Not Hope   \n",
       "3  #EspnF90 el var se cre√≥ para ayudar que los √°r...  Not Hope   \n",
       "4  hay un personaje de la primera peli que me rec...  Not Hope   \n",
       "\n",
       "         multiclass  \n",
       "0  Generalized Hope  \n",
       "1          Not Hope  \n",
       "2          Not Hope  \n",
       "3          Not Hope  \n",
       "4          Not Hope  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"SHSD.csv\")\n",
    "print(\"Dataset loaded:\", len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6efce8a-e7b6-46a9-955c-8eae9bada432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary subset: 1000\n",
      "Multiclass subset: 1002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Theas\\AppData\\Local\\Temp\\ipykernel_34948\\865934801.py:9: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(500, random_state=42))\n",
      "C:\\Users\\Theas\\AppData\\Local\\Temp\\ipykernel_34948\\865934801.py:26: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: x.sample(334, random_state=42))\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Subset BINARIO (Hope vs Not Hope)\n",
    "# ============================\n",
    "subset = df[df['binary'].isin(['Not Hope', 'Hope'])]\n",
    "\n",
    "subsetBin = (\n",
    "    subset\n",
    "    .groupby('binary', group_keys=False)\n",
    "    .apply(lambda x: x.sample(500, random_state=42))\n",
    "    .sample(frac=1, random_state=42)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# Subset MULTICLASS\n",
    "# ============================\n",
    "subset2 = df[df['multiclass'].isin([\n",
    "    'Generalized Hope',\n",
    "    'Realistic Hope',\n",
    "    'Unrealistic Hope'\n",
    "])]\n",
    "\n",
    "subsetMulti = (\n",
    "    subset2\n",
    "    .groupby('multiclass', group_keys=False)\n",
    "    .apply(lambda x: x.sample(334, random_state=42))\n",
    "    .sample(frac=1, random_state=42)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"Binary subset:\", len(subsetBin))\n",
    "print(\"Multiclass subset:\", len(subsetMulti))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0fd2898-08bc-46d2-8085-447a13deee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificarBin_batch(lista_textos, prompt_version=1, df=None, model=None):\n",
    "    \"\"\"\n",
    "    Clasifica una LISTA de textos en:\n",
    "    - Esperanza\n",
    "    - No Esperanza\n",
    "\n",
    "    prompt_version:\n",
    "    1 ‚Üí Zero-shot\n",
    "    2 ‚Üí One-shot (1 ejemplo)\n",
    "    3 ‚Üí Few-shot (hasta 5 ejemplos)\n",
    "\n",
    "    Retorna SIEMPRE len(lista_textos) etiquetas.\n",
    "    \"\"\"\n",
    "\n",
    "    VALID_LABELS = [\"Esperanza\", \"No Esperanza\"]\n",
    "\n",
    "    definicion_hope = \"Esperanza significa expresar optimismo, confianza o el deseo de un futuro mejor.\"\n",
    "\n",
    "    # ===================== BLOQUE NUMERADO =====================\n",
    "    bloque_tweets = \"\\n\".join(\n",
    "        f\"{i+1}. {texto}\" for i, texto in enumerate(lista_textos)\n",
    "    )\n",
    "\n",
    "    # ===================== EJEMPLOS =====================\n",
    "    ejemplo_texto = \"\"\n",
    "    ejemplos_texto = \"\"\n",
    "\n",
    "    if df is not None and \"binary\" in df.columns and not df.empty:\n",
    "\n",
    "        # üîπ ONE-SHOT ‚Üí 1 ejemplo\n",
    "        if prompt_version == 2:\n",
    "            row = df.sample(1).iloc[0]\n",
    "            ejemplo_texto = f'\"{row[\"text\"]}\" ‚Üí {row[\"binary\"]}'\n",
    "\n",
    "        # üîπ FEW-SHOT ‚Üí hasta 5 ejemplos\n",
    "        elif prompt_version == 3:\n",
    "            ejemplos = df.sample(min(5, len(df)))\n",
    "            ejemplos_texto = \"\\n\".join(\n",
    "                f'\"{row[\"text\"]}\" ‚Üí {row[\"binary\"]}'\n",
    "                for _, row in ejemplos.iterrows()\n",
    "            )\n",
    "\n",
    "    # Fallback si no hay df\n",
    "    if prompt_version == 2 and ejemplo_texto == \"\":\n",
    "        ejemplo_texto = '\"Las cosas mejorar√°n pronto.\" ‚Üí Esperanza'\n",
    "\n",
    "    if prompt_version == 3 and ejemplos_texto == \"\":\n",
    "        ejemplos_texto = '\"Las cosas mejorar√°n pronto.\" ‚Üí Esperanza'\n",
    "\n",
    "    # ===================== PROMPTS =====================\n",
    "\n",
    "    # ---------- ZERO SHOT ----------\n",
    "    if prompt_version == 1:\n",
    "        prompt = f\"\"\"\n",
    "Definici√≥n: {definicion_hope}\n",
    "\n",
    "Recibir√°s una lista numerada de tuits.\n",
    "Devuelve EXACTAMENTE UNA ETIQUETA POR L√çNEA, en el mismo orden.\n",
    "Etiquetas v√°lidas:\n",
    "- Esperanza\n",
    "- No Esperanza\n",
    "\n",
    "Tuits:\n",
    "{bloque_tweets}\n",
    "\n",
    "Ahora devuelve SOLO las etiquetas, una por l√≠nea, sin n√∫meros y sin texto adicional:\n",
    "\"\"\"\n",
    "\n",
    "    # ---------- ONE SHOT (1 ejemplo) ----------\n",
    "    elif prompt_version == 2:\n",
    "        prompt = f\"\"\"\n",
    "Definici√≥n: {definicion_hope}\n",
    "\n",
    "Ejemplo:\n",
    "{ejemplo_texto}\n",
    "\n",
    "Recibir√°s una lista numerada de tuits.\n",
    "Devuelve EXACTAMENTE UNA ETIQUETA POR L√çNEA, en el mismo orden.\n",
    "Etiquetas v√°lidas:\n",
    "- Esperanza\n",
    "- No Esperanza\n",
    "\n",
    "Tuits:\n",
    "{bloque_tweets}\n",
    "\n",
    "Ahora devuelve SOLO las etiquetas, una por l√≠nea, sin n√∫meros y sin texto adicional:\n",
    "\"\"\"\n",
    "\n",
    "    # ---------- FEW SHOT (5 ejemplos) ----------\n",
    "    else:\n",
    "        prompt = f\"\"\"\n",
    "Definici√≥n: {definicion_hope}\n",
    "\n",
    "Ejemplos:\n",
    "{ejemplos_texto}\n",
    "\n",
    "Recibir√°s una lista numerada de tuits.\n",
    "Devuelve EXACTAMENTE UNA ETIQUETA POR L√çNEA, en el mismo orden.\n",
    "Etiquetas v√°lidas:\n",
    "- Esperanza\n",
    "- No Esperanza\n",
    "\n",
    "Tuits:\n",
    "{bloque_tweets}\n",
    "\n",
    "Ahora devuelve SOLO las etiquetas, una por l√≠nea, sin n√∫meros y sin texto adicional:\n",
    "\"\"\"\n",
    "\n",
    "    # ===================== CALL MODEL =====================\n",
    "    raw = groq_generate(prompt, model=model)\n",
    "    raw_lines = [l.strip() for l in raw.splitlines() if l.strip()]\n",
    "\n",
    "    # ===================== PARSER ROBUSTO =====================\n",
    "    labels = []\n",
    "\n",
    "    for line in raw_lines:\n",
    "        clean = line.strip()\n",
    "\n",
    "        if clean == \"\":\n",
    "            continue\n",
    "        if clean.lower().startswith((\"tweet\", \"label\", \"output\")):\n",
    "            continue\n",
    "        if \":\" in clean:\n",
    "            clean = clean.split(\":\")[-1].strip()\n",
    "\n",
    "        for v in VALID_LABELS:\n",
    "            if v.lower() == clean.lower():\n",
    "                labels.append(v)\n",
    "                break\n",
    "\n",
    "    # ===================== FALLBACK SEGURO =====================\n",
    "    if len(labels) < len(lista_textos):\n",
    "        labels += [\"No Esperanza\"] * (len(lista_textos) - len(labels))\n",
    "    elif len(labels) > len(lista_textos):\n",
    "        labels = labels[:len(lista_textos)]\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4fe8fc2-a076-439f-9075-309fabbf4279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificarMulti_batch(lista_textos, prompt_version=1, df=None, model=None):\n",
    "    \"\"\"\n",
    "    Clasifica textos usando SOLO LAS ETIQUETAS DEL PROMPT:\n",
    "    - Esperanza Abstracta\n",
    "    - Esperanza Realista\n",
    "    - Deseo Fant√°stico\n",
    "\n",
    "    ‚ùó No aparecen etiquetas del dataset aqu√≠.\n",
    "    \"\"\"\n",
    "\n",
    "    PROMPT_LABELS = [\n",
    "        \"Esperanza Abstracta\",\n",
    "        \"Esperanza Realista\",\n",
    "        \"Deseo Fant√°stico\"\n",
    "    ]\n",
    "\n",
    "    # ---------- Dataset para shots (mapeado) ----------\n",
    "    df_hope = None\n",
    "    if df is not None and \"multiclass\" in df.columns:\n",
    "        df_hope = df.copy()\n",
    "        df_hope[\"prompt_label\"] = df_hope[\"multiclass\"].map({\n",
    "            \"Generalized Hope\": \"Esperanza Abstracta\",\n",
    "            \"Realistic Hope\": \"Esperanza Realista\",\n",
    "            \"Unrealistic Hope\": \"Deseo Fant√°stico\"\n",
    "        })\n",
    "        df_hope = df_hope[df_hope[\"prompt_label\"].notna()]\n",
    "\n",
    "    bloque_tweets = \"\\n\".join(f\"{i+1}. {t}\" for i, t in enumerate(lista_textos))\n",
    "\n",
    "    RULES = \"\"\"\n",
    "REGLAS DE CLASIFICACI√ìN:\n",
    "\n",
    "DESEO FANT√ÅSTICO:\n",
    "- Resultados imposibles, milagrosos, m√°gicos o irreales.\n",
    "- Deseos ficticios, c√≥smicos o claramente exagerados.\n",
    "\n",
    "ESPERANZA REALISTA:\n",
    "- Esperanza claramente ligada a una situaci√≥n o evento del mundo real.\n",
    "- Asociada a un contexto concreto (evento, fecha, persona, instituci√≥n).\n",
    "- Plausible dentro de la realidad normal.\n",
    "\n",
    "ESPERANZA ABSTRACTA:\n",
    "- Expresiones generales, emocionales o simb√≥licas de esperanza.\n",
    "- Bendiciones, oraciones, mensajes de √°nimo u optimismo vago.\n",
    "- NO claramente asociada a un evento o situaci√≥n espec√≠fica.\n",
    "\"\"\"\n",
    "\n",
    "    # ---------- PROMPTS ----------\n",
    "    if prompt_version == 1:  # Zero-shot\n",
    "        prompt = f\"\"\"\n",
    "Clasifica cada tuit en UNA de las siguientes categor√≠as:\n",
    "- Esperanza Abstracta\n",
    "- Esperanza Realista\n",
    "- Deseo Fant√°stico\n",
    "\n",
    "{RULES}\n",
    "\n",
    "Tuits:\n",
    "{bloque_tweets}\n",
    "\n",
    "Devuelve SOLO una etiqueta por l√≠nea:\n",
    "\"\"\"\n",
    "\n",
    "    elif prompt_version == 2:  # One-shot\n",
    "        ex = df_hope.sample(1).iloc[0] if df_hope is not None else None\n",
    "        ej_text = ex[\"text\"] if ex is not None else \"Espero que un milagro arregle todo de la noche a la ma√±ana.\"\n",
    "        ej_label = ex[\"prompt_label\"] if ex is not None else \"Deseo Fant√°stico\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Clasifica cada tuit en UNA de las siguientes categor√≠as:\n",
    "- Esperanza Abstracta\n",
    "- Esperanza Realista\n",
    "- Deseo Fant√°stico\n",
    "\n",
    "{RULES}\n",
    "\n",
    "Ejemplo:\n",
    "\"{ej_text}\" ‚Üí {ej_label}\n",
    "\n",
    "Tuits:\n",
    "{bloque_tweets}\n",
    "\n",
    "Devuelve SOLO una etiqueta por l√≠nea:\n",
    "\"\"\"\n",
    "\n",
    "    else:  # Few-shot\n",
    "        ejemplos = []\n",
    "        if df_hope is not None:\n",
    "            for lbl in PROMPT_LABELS:\n",
    "                sub = df_hope[df_hope[\"prompt_label\"] == lbl]\n",
    "                if not sub.empty:\n",
    "                    r = sub.sample(1).iloc[0]\n",
    "                    ejemplos.append(f'\"{r[\"text\"]}\" ‚Üí {lbl}')\n",
    "\n",
    "        ejemplos_texto = \"\\n\".join(ejemplos)\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "Clasifica cada tuit en UNA de las siguientes categor√≠as:\n",
    "- Esperanza Abstracta\n",
    "- Esperanza Realista\n",
    "- Deseo Fant√°stico\n",
    "\n",
    "{RULES}\n",
    "\n",
    "Ejemplos:\n",
    "{ejemplos_texto}\n",
    "\n",
    "Tuits:\n",
    "{bloque_tweets}\n",
    "\n",
    "Devuelve SOLO una etiqueta por l√≠nea:\n",
    "\"\"\"\n",
    "\n",
    "    raw = groq_generate(prompt, model=model)\n",
    "    raw_lines = [l.strip() for l in raw.splitlines() if l.strip()]\n",
    "\n",
    "    def normalize(label):\n",
    "        t = label.lower()\n",
    "        if \"fant√°stico\" in t or \"fantastico\" in t or \"deseo\" in t:\n",
    "            return \"Deseo Fant√°stico\"\n",
    "        if \"realista\" in t:\n",
    "            return \"Esperanza Realista\"\n",
    "        if \"abstracta\" in t or \"abstracto\" in t:\n",
    "            return \"Esperanza Abstracta\"\n",
    "        return \"Esperanza Abstracta\"\n",
    "\n",
    "    labels = [normalize(l) for l in raw_lines]\n",
    "\n",
    "    if len(labels) < len(lista_textos):\n",
    "        labels += [\"Esperanza Abstracta\"] * (len(lista_textos) - len(labels))\n",
    "    elif len(labels) > len(lista_textos):\n",
    "        labels = labels[:len(lista_textos)]\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fd89ced-629d-4e1f-a5ab-15d68eb24ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    MODELS = {\n",
    "        # ---------- LLaMA family ----------\n",
    "        \"LLaMA-8B\": \"llama-3.1-8b-instant\",\n",
    "        \"LLaMA-70B\": \"llama-3.3-70b-versatile\",\n",
    "    \n",
    "        # ---------- LLaMA-4 (Instruct variants) ----------\n",
    "        \"Maverick-17B\": \"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "        \"Scout-17B\": \"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    \n",
    "        # ---------- External / Non-LLaMA ----------\n",
    "        \"Allam-7B\": \"allam-2-7b\",\n",
    "        \"Kimi-K2\": \"moonshotai/kimi-k2-instruct\",\n",
    "        \"Qwen-32B\": \"qwen/qwen3-32b\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "904d8976-694c-4be4-8e1d-20043da17eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≠Ô∏è Skipping models: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ade507b24d4bafaafb3f4af891855e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Modelos (Binary):   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      "Testing MODEL (Binary): LLaMA-8B\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190115a5f71f4535b9d47d8a0dd5ad08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prompts (LLaMA-8B):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8afe91171ade44bc81c444d9419b7ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binary p1 ‚Äî LLaMA-8B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-8B ‚Äî Binary Prompt 1 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb302b991fe541f48aef367f193738c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binary p2 ‚Äî LLaMA-8B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-8B ‚Äî Binary Prompt 2 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ab8c8bce8449ba87fa155c0bbff0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binary p3 ‚Äî LLaMA-8B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-8B ‚Äî Binary Prompt 3 DONE.\n",
      "\n",
      "==========================\n",
      "Testing MODEL (Binary): LLaMA-70B\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5f96e2074814af9acee687b47cb751c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prompts (LLaMA-70B):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44f7f905fc97457fa6821d028a6890df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binary p1 ‚Äî LLaMA-70B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-70B ‚Äî Binary Prompt 1 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d00a0860f94b3c837644f54729843a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binary p2 ‚Äî LLaMA-70B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-70B ‚Äî Binary Prompt 2 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b95bbf69ea403cb1bf867fff50f62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binary p3 ‚Äî LLaMA-70B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-70B ‚Äî Binary Prompt 3 DONE.\n",
      "\n",
      "==========================\n",
      "Testing MODEL (Binary): Maverick-17B\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14134292a8841f9b14cb0f91121ecb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prompts (Maverick-17B):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478cc4ebd4ea442a874e194ba506d694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binary p1 ‚Äî Maverick-17B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maverick-17B ‚Äî Binary Prompt 1 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370b51cf897d41ca875018bd2f312d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binary p2 ‚Äî Maverick-17B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maverick-17B ‚Äî Binary Prompt 2 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c7f8065166492d97313d874b06048f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binary p3 ‚Äî Maverick-17B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maverick-17B ‚Äî Binary Prompt 3 DONE.\n",
      "\n",
      "==========================\n",
      "Testing MODEL (Binary): Scout-17B\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f3c691359044a17b7168fef5374f806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prompts (Scout-17B):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c91a08f405946bab797db185d63dda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binary p1 ‚Äî Scout-17B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scout-17B ‚Äî Binary Prompt 1 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947594ad158c4d8ebd5395a915fa3501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binary p2 ‚Äî Scout-17B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scout-17B ‚Äî Binary Prompt 2 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361480647a54427a9d0694cde8e62a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binary p3 ‚Äî Scout-17B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allam-7B ‚Äî Binary Prompt 1 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5b12f1d3f546218ed798c2540fa28d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binary p2 ‚Äî Allam-7B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allam-7B ‚Äî Binary Prompt 2 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165c7a3d37ef4448bc01830cb27cfa0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binary p3 ‚Äî Allam-7B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allam-7B ‚Äî Binary Prompt 3 DONE.\n",
      "\n",
      "==========================\n",
      "Testing MODEL (Binary): Kimi-K2\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4c237a20d9c41a180f0b0a9e900aa29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prompts (Kimi-K2):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb16502262d49839a031d1ab0af8afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binary p1 ‚Äî Kimi-K2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kimi-K2 ‚Äî Binary Prompt 1 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b01ffa3c064db2875372f0a184ae8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binary p2 ‚Äî Kimi-K2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kimi-K2 ‚Äî Binary Prompt 2 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51beb100b731487f922cc1ae5616de6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binary p3 ‚Äî Kimi-K2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kimi-K2 ‚Äî Binary Prompt 3 DONE.\n",
      "\n",
      "==========================\n",
      "Testing MODEL (Binary): Qwen-32B\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f382074b9b4a57938f535532d0fa52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prompts (Qwen-32B):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06cd630aad64168b49f0cf42e45e14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binary p1 ‚Äî Qwen-32B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen-32B ‚Äî Binary Prompt 1 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749bc1271ad1464c9a1647a144e7a5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binary p2 ‚Äî Qwen-32B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen-32B ‚Äî Binary Prompt 2 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2028ee220aa24046a8342849db364632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Binary p3 ‚Äî Qwen-32B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen-32B ‚Äî Binary Prompt 3 DONE.\n",
      "\n",
      "‚úÖ Evaluaci√≥n BINARIA terminada. CSV guardado correctamente.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from groq import RateLimitError\n",
    "import pandas as pd\n",
    "\n",
    "RESULTS = []\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "NUM_RECORDS = 200\n",
    "\n",
    "finBin = subsetBin.sample(NUM_RECORDS).copy()  # ‚úÖ SOLO binary\n",
    "\n",
    "# ‚õî MODELOS A SALTAR\n",
    "SKIP_MODELS = []\n",
    "\n",
    "print(\"‚è≠Ô∏è Skipping models:\", SKIP_MODELS)\n",
    "\n",
    "# ===================== LOOP PRINCIPAL =====================\n",
    "for model_name, model_id in tqdm(\n",
    "    MODELS.items(),\n",
    "    desc=\"Modelos (Binary)\",\n",
    "    total=len(MODELS)\n",
    "):\n",
    "\n",
    "    if model_name in SKIP_MODELS:\n",
    "        print(f\"\\n‚è≠Ô∏è SKIPPING MODEL (hardcoded): {model_name}\\n\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n==========================\")\n",
    "    print(f\"Testing MODEL (Binary): {model_name}\")\n",
    "    print(\"==========================\\n\")\n",
    "\n",
    "    try:\n",
    "        for prompt_version in tqdm(\n",
    "            [1, 2, 3],\n",
    "            desc=f\"Prompts ({model_name})\"\n",
    "        ):\n",
    "\n",
    "            col = f\"{model_name}_p{prompt_version}\"\n",
    "            if col in finBin.columns:\n",
    "                print(f\"‚è≠Ô∏è {col} already exists, skipping\")\n",
    "                continue\n",
    "\n",
    "            preds_bin = []\n",
    "            batches_bin = range(0, len(finBin), BATCH_SIZE)\n",
    "\n",
    "            for i in tqdm(\n",
    "                batches_bin,\n",
    "                desc=f\"Binary p{prompt_version} ‚Äî {model_name}\",\n",
    "                leave=False\n",
    "            ):\n",
    "                batch = finBin[\"text\"].iloc[i:i+BATCH_SIZE].tolist()\n",
    "                preds_bin.extend(\n",
    "                    clasificarBin_batch(\n",
    "                        batch,\n",
    "                        prompt_version,\n",
    "                        df=subsetBin,\n",
    "                        model=model_id\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            finBin[col] = preds_bin\n",
    "            print(f\"{model_name} ‚Äî Binary Prompt {prompt_version} DONE.\")\n",
    "\n",
    "        # üîπ Guardado parcial por modelo\n",
    "        finBin.to_csv(f\"resultados_binary_{model_name}.csv\", index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Error en modelo {model_name}: {e}\")\n",
    "        print(\"‚è≠Ô∏è Saltando este modelo y continuando.\\n\")\n",
    "        continue\n",
    "\n",
    "# ===================== GUARDADO FINAL =====================\n",
    "finBin.to_csv(\"Btempraw.csv\", index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Evaluaci√≥n BINARIA terminada. CSV guardado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e55bedf-da29-4ff4-8b59-cb6924e8ea3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≠Ô∏è Skipping models: []\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7971372fc644dfa7b92022c90109b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Modelos:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================\n",
      "Testing MODEL: LLaMA-8B\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e90a005c4da4a149c856dcc827d9f18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prompts (LLaMA-8B):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d71c2349be4e189792da257b2f1e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p1 ‚Äî LLaMA-8B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-8B ‚Äî Prompt 1 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e51efb8fb14cd29a8da26670da57ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p2 ‚Äî LLaMA-8B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-8B ‚Äî Prompt 2 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f464698151e9445cad3f708df066ef25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p3 ‚Äî LLaMA-8B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-8B ‚Äî Prompt 3 DONE.\n",
      "\n",
      "==========================\n",
      "Testing MODEL: LLaMA-70B\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fcb81d06a7043c1a233001999632eac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prompts (LLaMA-70B):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3d84268c514f4e841c29e3d9765370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p1 ‚Äî LLaMA-70B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-70B ‚Äî Prompt 1 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0846d7f65c4547dbb474760aafd90a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p2 ‚Äî LLaMA-70B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-70B ‚Äî Prompt 2 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fea5aa25db549bea444dde02355126f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p3 ‚Äî LLaMA-70B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMA-70B ‚Äî Prompt 3 DONE.\n",
      "\n",
      "==========================\n",
      "Testing MODEL: Maverick-17B\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18e5840bf88144e6a2cd510d2ef876ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prompts (Maverick-17B):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5500e3c63794a948aa80c448de11431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p1 ‚Äî Maverick-17B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maverick-17B ‚Äî Prompt 1 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ce290011044423ab0e2147179fd523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p2 ‚Äî Maverick-17B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maverick-17B ‚Äî Prompt 2 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8693d9a4bad54edfb1a5e60e76386e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p3 ‚Äî Maverick-17B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maverick-17B ‚Äî Prompt 3 DONE.\n",
      "\n",
      "==========================\n",
      "Testing MODEL: Scout-17B\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad90dfba0e2f48b2b0c343647b128369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prompts (Scout-17B):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acfd71c90c6c4823817c0da2d0855531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p1 ‚Äî Scout-17B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scout-17B ‚Äî Prompt 1 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32709114c4e04fdf82a30c38b39ec00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p2 ‚Äî Scout-17B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scout-17B ‚Äî Prompt 2 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24753e879d9c4baeb6cbdfb7b0bc8f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p3 ‚Äî Scout-17B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scout-17B ‚Äî Prompt 3 DONE.\n",
      "\n",
      "==========================\n",
      "Testing MODEL: Allam-7B\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a463b7e2e3b0400ab898a23328aa872f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prompts (Allam-7B):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f5a48588fa44599c3353b564e6019b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p1 ‚Äî Allam-7B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allam-7B ‚Äî Prompt 1 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b411cc6a49a43eaa51397db1706c8e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p2 ‚Äî Allam-7B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allam-7B ‚Äî Prompt 2 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095aa1f3cf3f42c58e1ec2e8c80bec60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p3 ‚Äî Allam-7B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allam-7B ‚Äî Prompt 3 DONE.\n",
      "\n",
      "==========================\n",
      "Testing MODEL: Kimi-K2\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b27e1d5e3884b738758bfa097f2b1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prompts (Kimi-K2):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ad1338eb894b33aa8490385298de02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p1 ‚Äî Kimi-K2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kimi-K2 ‚Äî Prompt 1 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "244a2f2bd923402a905d8d6d2e98052f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p2 ‚Äî Kimi-K2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kimi-K2 ‚Äî Prompt 2 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b3f67bcd3a43108db85fbb03c9e7e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p3 ‚Äî Kimi-K2:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kimi-K2 ‚Äî Prompt 3 DONE.\n",
      "\n",
      "==========================\n",
      "Testing MODEL: Qwen-32B\n",
      "==========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28462a17bd65454795e5e3156cbda010",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Prompts (Qwen-32B):   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6daf1881e57846aaa1be6c250f4d1584",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p1 ‚Äî Qwen-32B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen-32B ‚Äî Prompt 1 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c15ef4ad5b04af09a7798c627048c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p2 ‚Äî Qwen-32B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen-32B ‚Äî Prompt 2 DONE.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a84756e1bdc48739cca217b98a4385a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Multi p3 ‚Äî Qwen-32B:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen-32B ‚Äî Prompt 3 DONE.\n",
      "\n",
      "‚úÖ Evaluaci√≥n terminada. CSV guardado correctamente.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from groq import RateLimitError\n",
    "import pandas as pd\n",
    "\n",
    "RESULTS = []\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "NUM_RECORDS = 200\n",
    "\n",
    "finMulti = subsetMulti.sample(NUM_RECORDS).copy()\n",
    "\n",
    "SKIP_MODELS = []\n",
    "print(\"‚è≠Ô∏è Skipping models:\", SKIP_MODELS)\n",
    "\n",
    "for model_name, model_id in tqdm(MODELS.items(), desc=\"Modelos\", total=len(MODELS)):\n",
    "\n",
    "    if model_name in SKIP_MODELS:\n",
    "        print(f\"\\n‚è≠Ô∏è SKIPPING MODEL (hardcoded): {model_name}\\n\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n==========================\")\n",
    "    print(f\"Testing MODEL: {model_name}\")\n",
    "    print(\"==========================\\n\")\n",
    "\n",
    "    try:\n",
    "        for prompt_version in tqdm([1, 2, 3], desc=f\"Prompts ({model_name})\"):\n",
    "\n",
    "            col = f\"{model_name}_p{prompt_version}\"\n",
    "            if col in finMulti.columns:\n",
    "                print(f\"‚è≠Ô∏è {col} already exists, skipping\")\n",
    "                continue\n",
    "\n",
    "            preds_multi = []\n",
    "            batches_multi = range(0, len(finMulti), BATCH_SIZE)\n",
    "\n",
    "            for i in tqdm(\n",
    "                batches_multi,\n",
    "                desc=f\"Multi p{prompt_version} ‚Äî {model_name}\",\n",
    "                leave=False\n",
    "            ):\n",
    "                batch = finMulti[\"text\"].iloc[i:i+BATCH_SIZE].tolist()\n",
    "                preds_multi.extend(\n",
    "                    clasificarMulti_batch(\n",
    "                        batch,\n",
    "                        prompt_version,\n",
    "                        df=subsetMulti,\n",
    "                        model=model_id\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            finMulti[col] = preds_multi\n",
    "            print(f\"{model_name} ‚Äî Prompt {prompt_version} DONE.\")\n",
    "\n",
    "        finMulti.to_csv(f\"resultados_multiclass_{model_name}.csv\", index=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Error en modelo {model_name}: {e}\")\n",
    "        print(\"‚è≠Ô∏è Saltando este modelo y continuando.\\n\")\n",
    "        continue\n",
    "\n",
    "finMulti.to_csv(\"Mtempraw.csv\", index=False)\n",
    "print(\"\\n‚úÖ Evaluaci√≥n terminada. CSV guardado correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f745065-71ee-42a1-98b3-639a4d6aae34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== üß† BINARY CLASSIFICATION (ALL MODELS) =====\n",
      "\n",
      "======================\n",
      "MODEL: LLaMA-8B\n",
      "======================\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Accuracy: 57.50%\n",
      "Macro F1 (PRIMARY): 0.5683\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hope       0.56      0.70      0.62       100\n",
      "    Not Hope       0.60      0.45      0.51       100\n",
      "\n",
      "    accuracy                           0.57       200\n",
      "   macro avg       0.58      0.57      0.57       200\n",
      "weighted avg       0.58      0.57      0.57       200\n",
      "\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Accuracy: 56.50%\n",
      "Macro F1 (PRIMARY): 0.5581\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hope       0.55      0.69      0.61       100\n",
      "    Not Hope       0.59      0.44      0.50       100\n",
      "\n",
      "    accuracy                           0.56       200\n",
      "   macro avg       0.57      0.56      0.56       200\n",
      "weighted avg       0.57      0.56      0.56       200\n",
      "\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Accuracy: 58.50%\n",
      "Macro F1 (PRIMARY): 0.5812\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hope       0.57      0.68      0.62       100\n",
      "    Not Hope       0.60      0.49      0.54       100\n",
      "\n",
      "    accuracy                           0.58       200\n",
      "   macro avg       0.59      0.58      0.58       200\n",
      "weighted avg       0.59      0.58      0.58       200\n",
      "\n",
      "\n",
      "======================\n",
      "MODEL: LLaMA-70B\n",
      "======================\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Accuracy: 73.50%\n",
      "Macro F1 (PRIMARY): 0.7347\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hope       0.75      0.70      0.73       100\n",
      "    Not Hope       0.72      0.77      0.74       100\n",
      "\n",
      "    accuracy                           0.73       200\n",
      "   macro avg       0.74      0.73      0.73       200\n",
      "weighted avg       0.74      0.73      0.73       200\n",
      "\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Accuracy: 71.50%\n",
      "Macro F1 (PRIMARY): 0.7141\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hope       0.74      0.66      0.70       100\n",
      "    Not Hope       0.69      0.77      0.73       100\n",
      "\n",
      "    accuracy                           0.71       200\n",
      "   macro avg       0.72      0.72      0.71       200\n",
      "weighted avg       0.72      0.71      0.71       200\n",
      "\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Accuracy: 70.00%\n",
      "Macro F1 (PRIMARY): 0.7000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hope       0.70      0.69      0.70       100\n",
      "    Not Hope       0.70      0.71      0.70       100\n",
      "\n",
      "    accuracy                           0.70       200\n",
      "   macro avg       0.70      0.70      0.70       200\n",
      "weighted avg       0.70      0.70      0.70       200\n",
      "\n",
      "\n",
      "======================\n",
      "MODEL: Maverick-17B\n",
      "======================\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Accuracy: 78.00%\n",
      "Macro F1 (PRIMARY): 0.7798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hope       0.76      0.81      0.79       100\n",
      "    Not Hope       0.80      0.75      0.77       100\n",
      "\n",
      "    accuracy                           0.78       200\n",
      "   macro avg       0.78      0.78      0.78       200\n",
      "weighted avg       0.78      0.78      0.78       200\n",
      "\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Accuracy: 76.00%\n",
      "Macro F1 (PRIMARY): 0.7600\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hope       0.77      0.75      0.76       100\n",
      "    Not Hope       0.75      0.77      0.76       100\n",
      "\n",
      "    accuracy                           0.76       200\n",
      "   macro avg       0.76      0.76      0.76       200\n",
      "weighted avg       0.76      0.76      0.76       200\n",
      "\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Accuracy: 77.50%\n",
      "Macro F1 (PRIMARY): 0.7740\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hope       0.74      0.84      0.79       100\n",
      "    Not Hope       0.82      0.71      0.76       100\n",
      "\n",
      "    accuracy                           0.78       200\n",
      "   macro avg       0.78      0.77      0.77       200\n",
      "weighted avg       0.78      0.78      0.77       200\n",
      "\n",
      "\n",
      "======================\n",
      "MODEL: Scout-17B\n",
      "======================\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Accuracy: 70.50%\n",
      "Macro F1 (PRIMARY): 0.6945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hope       0.83      0.52      0.64       100\n",
      "    Not Hope       0.65      0.89      0.75       100\n",
      "\n",
      "    accuracy                           0.70       200\n",
      "   macro avg       0.74      0.71      0.69       200\n",
      "weighted avg       0.74      0.70      0.69       200\n",
      "\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Accuracy: 73.50%\n",
      "Macro F1 (PRIMARY): 0.7314\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hope       0.81      0.62      0.70       100\n",
      "    Not Hope       0.69      0.85      0.76       100\n",
      "\n",
      "    accuracy                           0.73       200\n",
      "   macro avg       0.75      0.73      0.73       200\n",
      "weighted avg       0.75      0.73      0.73       200\n",
      "\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Accuracy: 72.50%\n",
      "Macro F1 (PRIMARY): 0.7213\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hope       0.79      0.61      0.69       100\n",
      "    Not Hope       0.68      0.84      0.75       100\n",
      "\n",
      "    accuracy                           0.72       200\n",
      "   macro avg       0.74      0.72      0.72       200\n",
      "weighted avg       0.74      0.72      0.72       200\n",
      "\n",
      "\n",
      "======================\n",
      "MODEL: Allam-7B\n",
      "======================\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Accuracy: 44.50%\n",
      "Macro F1 (PRIMARY): 0.4181\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hope       0.40      0.23      0.29       100\n",
      "    Not Hope       0.46      0.66      0.54       100\n",
      "\n",
      "    accuracy                           0.45       200\n",
      "   macro avg       0.43      0.45      0.42       200\n",
      "weighted avg       0.43      0.45      0.42       200\n",
      "\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Accuracy: 50.50%\n",
      "Macro F1 (PRIMARY): 0.4336\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hope       0.52      0.15      0.23       100\n",
      "    Not Hope       0.50      0.86      0.63       100\n",
      "\n",
      "    accuracy                           0.51       200\n",
      "   macro avg       0.51      0.51      0.43       200\n",
      "weighted avg       0.51      0.51      0.43       200\n",
      "\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Accuracy: 48.50%\n",
      "Macro F1 (PRIMARY): 0.3954\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hope       0.43      0.10      0.16       100\n",
      "    Not Hope       0.49      0.87      0.63       100\n",
      "\n",
      "    accuracy                           0.48       200\n",
      "   macro avg       0.46      0.48      0.40       200\n",
      "weighted avg       0.46      0.48      0.40       200\n",
      "\n",
      "\n",
      "======================\n",
      "MODEL: Kimi-K2\n",
      "======================\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Accuracy: 77.50%\n",
      "Macro F1 (PRIMARY): 0.7749\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hope       0.79      0.75      0.77       100\n",
      "    Not Hope       0.76      0.80      0.78       100\n",
      "\n",
      "    accuracy                           0.78       200\n",
      "   macro avg       0.78      0.78      0.77       200\n",
      "weighted avg       0.78      0.78      0.77       200\n",
      "\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Accuracy: 78.50%\n",
      "Macro F1 (PRIMARY): 0.7850\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hope       0.78      0.79      0.79       100\n",
      "    Not Hope       0.79      0.78      0.78       100\n",
      "\n",
      "    accuracy                           0.79       200\n",
      "   macro avg       0.79      0.79      0.78       200\n",
      "weighted avg       0.79      0.79      0.78       200\n",
      "\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Accuracy: 78.50%\n",
      "Macro F1 (PRIMARY): 0.7846\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hope       0.81      0.74      0.77       100\n",
      "    Not Hope       0.76      0.83      0.79       100\n",
      "\n",
      "    accuracy                           0.79       200\n",
      "   macro avg       0.79      0.78      0.78       200\n",
      "weighted avg       0.79      0.79      0.78       200\n",
      "\n",
      "\n",
      "======================\n",
      "MODEL: Qwen-32B\n",
      "======================\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Accuracy: 50.00%\n",
      "Macro F1 (PRIMARY): 0.3333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hope       0.00      0.00      0.00       100\n",
      "    Not Hope       0.50      1.00      0.67       100\n",
      "\n",
      "    accuracy                           0.50       200\n",
      "   macro avg       0.25      0.50      0.33       200\n",
      "weighted avg       0.25      0.50      0.33       200\n",
      "\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Accuracy: 50.00%\n",
      "Macro F1 (PRIMARY): 0.3333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hope       0.00      0.00      0.00       100\n",
      "    Not Hope       0.50      1.00      0.67       100\n",
      "\n",
      "    accuracy                           0.50       200\n",
      "   macro avg       0.25      0.50      0.33       200\n",
      "weighted avg       0.25      0.50      0.33       200\n",
      "\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Accuracy: 50.00%\n",
      "Macro F1 (PRIMARY): 0.3333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Hope       0.00      0.00      0.00       100\n",
      "    Not Hope       0.50      1.00      0.67       100\n",
      "\n",
      "    accuracy                           0.50       200\n",
      "   macro avg       0.25      0.50      0.33       200\n",
      "weighted avg       0.25      0.50      0.33       200\n",
      "\n",
      "\n",
      "‚úÖ Saved: Btemp.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"===== üß† BINARY CLASSIFICATION (ALL MODELS) =====\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "def normalize_binary(label):\n",
    "    \"\"\"\n",
    "    Normaliza salidas del modelo (en espa√±ol o ingl√©s)\n",
    "    y las mapea a:\n",
    "    - Hope\n",
    "    - Not Hope\n",
    "    \"\"\"\n",
    "    if not isinstance(label, str):\n",
    "        return \"Not Hope\"\n",
    "\n",
    "    t = label.strip().lower()\n",
    "\n",
    "    # Espa√±ol\n",
    "    if \"no esperanza\" in t:\n",
    "        return \"Not Hope\"\n",
    "    if t == \"esperanza\":\n",
    "        return \"Hope\"\n",
    "    if \"esperanza\" in t and \"no\" not in t:\n",
    "        return \"Hope\"\n",
    "\n",
    "    # Ingl√©s\n",
    "    if \"not hope\" in t:\n",
    "        return \"Not Hope\"\n",
    "    if t == \"hope\":\n",
    "        return \"Hope\"\n",
    "    if \"hope\" in t and \"not\" not in t:\n",
    "        return \"Hope\"\n",
    "\n",
    "    return \"Not Hope\"\n",
    "\n",
    "os.makedirs(\"used_prompts\", exist_ok=True)\n",
    "\n",
    "for model_name in MODELS.keys():\n",
    "    print(f\"\\n======================\")\n",
    "    print(f\"MODEL: {model_name}\")\n",
    "    print(\"======================\")\n",
    "\n",
    "    for p in [1, 2, 3]:\n",
    "        col = f\"{model_name}_p{p}\"\n",
    "\n",
    "        if col not in finBin.columns:\n",
    "            print(f\"‚è≠Ô∏è Missing {col}, skipping\")\n",
    "            continue\n",
    "\n",
    "        finBin[f\"{col}_norm\"] = finBin[col].apply(normalize_binary)\n",
    "\n",
    "        # Ground truth en ingl√©s\n",
    "        y_true = finBin[\"binary\"]          # Hope / Not Hope\n",
    "        y_pred = finBin[f\"{col}_norm\"]\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        report = classification_report(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            output_dict=True,\n",
    "            zero_division=0\n",
    "        )\n",
    "\n",
    "        macro_f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "        invalid_outputs = sum(\n",
    "            1 for x in finBin[col]\n",
    "            if not isinstance(x, str)\n",
    "            or (\n",
    "                \"hope\" not in x.lower()\n",
    "                and \"esperanza\" not in x.lower()\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(f\"\\n--- Prompt {p} ---\")\n",
    "        print(f\"Accuracy: {acc*100:.2f}%\")\n",
    "        print(f\"Macro F1 (PRIMARY): {macro_f1:.4f}\")\n",
    "        print(classification_report(y_true, y_pred, zero_division=0))\n",
    "\n",
    "        for label in [\"Hope\", \"Not Hope\"]:\n",
    "            rows.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Prompt\": p,\n",
    "                \"PromptType\": [\"zero-shot\", \"one-shot\", \"few-shot\"][p-1],\n",
    "                \"Label\": label,\n",
    "                \"Precision\": report[label][\"precision\"],\n",
    "                \"Recall\": report[label][\"recall\"],\n",
    "                \"F1\": report[label][\"f1-score\"],\n",
    "                \"Support\": report[label][\"support\"],\n",
    "                \"Accuracy\": acc,\n",
    "                \"Primary_Macro_F1\": macro_f1,\n",
    "                \"InvalidOutputs\": invalid_outputs\n",
    "            })\n",
    "\n",
    "        rows.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Prompt\": p,\n",
    "            \"PromptType\": [\"zero-shot\", \"one-shot\", \"few-shot\"][p-1],\n",
    "            \"Label\": \"macro avg\",\n",
    "            \"Precision\": report[\"macro avg\"][\"precision\"],\n",
    "            \"Recall\": report[\"macro avg\"][\"recall\"],\n",
    "            \"F1\": macro_f1,\n",
    "            \"Support\": report[\"macro avg\"][\"support\"],\n",
    "            \"Accuracy\": acc,\n",
    "            \"Primary_Macro_F1\": macro_f1,\n",
    "            \"InvalidOutputs\": invalid_outputs\n",
    "        })\n",
    "\n",
    "        rows.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Prompt\": p,\n",
    "            \"PromptType\": [\"zero-shot\", \"one-shot\", \"few-shot\"][p-1],\n",
    "            \"Label\": \"weighted avg\",\n",
    "            \"Precision\": report[\"weighted avg\"][\"precision\"],\n",
    "            \"Recall\": report[\"weighted avg\"][\"recall\"],\n",
    "            \"F1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "            \"Support\": report[\"weighted avg\"][\"support\"],\n",
    "            \"Accuracy\": acc,\n",
    "            \"Primary_Macro_F1\": macro_f1,\n",
    "            \"InvalidOutputs\": invalid_outputs\n",
    "        })\n",
    "\n",
    "        with open(f\"used_prompts/binary_{model_name}_prompt{p}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"MODEL: {model_name}\\nPROMPT VERSION: {p}\\n\")\n",
    "\n",
    "df_binary_metrics = pd.DataFrame(rows)\n",
    "df_binary_metrics.to_csv(\"Btemp.csv\", index=False)\n",
    "\n",
    "print(\"\\n‚úÖ Saved: Btemp.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c23c5f7c-4c71-4a38-955a-c25e66441873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== üé® MULTICLASS CLASSIFICATION (ALL MODELS) =====\n",
      "\n",
      "======================\n",
      "MODEL: LLaMA-8B\n",
      "======================\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Accuracy: 45.50%\n",
      "Macro F1 (PRIMARY): 0.4253\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.40      0.75      0.52        72\n",
      "  Realistic Hope       0.55      0.34      0.42        68\n",
      "Unrealistic Hope       0.58      0.23      0.33        60\n",
      "\n",
      "        accuracy                           0.46       200\n",
      "       macro avg       0.51      0.44      0.43       200\n",
      "    weighted avg       0.51      0.46      0.43       200\n",
      "\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Accuracy: 46.00%\n",
      "Macro F1 (PRIMARY): 0.4370\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.41      0.71      0.52        72\n",
      "  Realistic Hope       0.53      0.38      0.44        68\n",
      "Unrealistic Hope       0.58      0.25      0.35        60\n",
      "\n",
      "        accuracy                           0.46       200\n",
      "       macro avg       0.51      0.45      0.44       200\n",
      "    weighted avg       0.50      0.46      0.44       200\n",
      "\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Accuracy: 43.50%\n",
      "Macro F1 (PRIMARY): 0.4012\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.40      0.75      0.52        72\n",
      "  Realistic Hope       0.50      0.21      0.29        68\n",
      "Unrealistic Hope       0.50      0.32      0.39        60\n",
      "\n",
      "        accuracy                           0.43       200\n",
      "       macro avg       0.47      0.42      0.40       200\n",
      "    weighted avg       0.47      0.43      0.40       200\n",
      "\n",
      "\n",
      "======================\n",
      "MODEL: LLaMA-70B\n",
      "======================\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Accuracy: 33.50%\n",
      "Macro F1 (PRIMARY): 0.2783\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.36      0.68      0.47        72\n",
      "  Realistic Hope       0.27      0.18      0.21        68\n",
      "Unrealistic Hope       0.33      0.10      0.15        60\n",
      "\n",
      "        accuracy                           0.34       200\n",
      "       macro avg       0.32      0.32      0.28       200\n",
      "    weighted avg       0.32      0.34      0.29       200\n",
      "\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Accuracy: 34.50%\n",
      "Macro F1 (PRIMARY): 0.2847\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.36      0.69      0.48        72\n",
      "  Realistic Hope       0.30      0.21      0.25        68\n",
      "Unrealistic Hope       0.29      0.08      0.13        60\n",
      "\n",
      "        accuracy                           0.34       200\n",
      "       macro avg       0.32      0.33      0.28       200\n",
      "    weighted avg       0.32      0.34      0.29       200\n",
      "\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Accuracy: 39.00%\n",
      "Macro F1 (PRIMARY): 0.3434\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.37      0.71      0.48        72\n",
      "  Realistic Hope       0.41      0.28      0.33        68\n",
      "Unrealistic Hope       0.53      0.13      0.21        60\n",
      "\n",
      "        accuracy                           0.39       200\n",
      "       macro avg       0.44      0.37      0.34       200\n",
      "    weighted avg       0.43      0.39      0.35       200\n",
      "\n",
      "\n",
      "======================\n",
      "MODEL: Maverick-17B\n",
      "======================\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Accuracy: 46.00%\n",
      "Macro F1 (PRIMARY): 0.4412\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.41      0.71      0.52        72\n",
      "  Realistic Hope       0.59      0.28      0.38        68\n",
      "Unrealistic Hope       0.50      0.37      0.42        60\n",
      "\n",
      "        accuracy                           0.46       200\n",
      "       macro avg       0.50      0.45      0.44       200\n",
      "    weighted avg       0.50      0.46      0.44       200\n",
      "\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Accuracy: 46.50%\n",
      "Macro F1 (PRIMARY): 0.4438\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.41      0.74      0.53        72\n",
      "  Realistic Hope       0.62      0.29      0.40        68\n",
      "Unrealistic Hope       0.51      0.33      0.40        60\n",
      "\n",
      "        accuracy                           0.47       200\n",
      "       macro avg       0.52      0.45      0.44       200\n",
      "    weighted avg       0.51      0.47      0.45       200\n",
      "\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Accuracy: 48.50%\n",
      "Macro F1 (PRIMARY): 0.4629\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.43      0.78      0.55        72\n",
      "  Realistic Hope       0.54      0.28      0.37        68\n",
      "Unrealistic Hope       0.65      0.37      0.47        60\n",
      "\n",
      "        accuracy                           0.48       200\n",
      "       macro avg       0.54      0.47      0.46       200\n",
      "    weighted avg       0.53      0.48      0.46       200\n",
      "\n",
      "\n",
      "======================\n",
      "MODEL: Scout-17B\n",
      "======================\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Accuracy: 34.00%\n",
      "Macro F1 (PRIMARY): 0.2875\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.36      0.68      0.47        72\n",
      "  Realistic Hope       0.25      0.09      0.13        68\n",
      "Unrealistic Hope       0.34      0.22      0.27        60\n",
      "\n",
      "        accuracy                           0.34       200\n",
      "       macro avg       0.32      0.33      0.29       200\n",
      "    weighted avg       0.32      0.34      0.29       200\n",
      "\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Accuracy: 34.00%\n",
      "Macro F1 (PRIMARY): 0.2799\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.36      0.71      0.47        72\n",
      "  Realistic Hope       0.22      0.07      0.11        68\n",
      "Unrealistic Hope       0.35      0.20      0.26        60\n",
      "\n",
      "        accuracy                           0.34       200\n",
      "       macro avg       0.31      0.33      0.28       200\n",
      "    weighted avg       0.31      0.34      0.28       200\n",
      "\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Accuracy: 34.00%\n",
      "Macro F1 (PRIMARY): 0.2760\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.36      0.72      0.48        72\n",
      "  Realistic Hope       0.22      0.07      0.11        68\n",
      "Unrealistic Hope       0.33      0.18      0.24        60\n",
      "\n",
      "        accuracy                           0.34       200\n",
      "       macro avg       0.30      0.33      0.28       200\n",
      "    weighted avg       0.30      0.34      0.28       200\n",
      "\n",
      "\n",
      "======================\n",
      "MODEL: Allam-7B\n",
      "======================\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Accuracy: 34.50%\n",
      "Macro F1 (PRIMARY): 0.2498\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.35      0.74      0.47        72\n",
      "  Realistic Hope       0.34      0.24      0.28        68\n",
      "Unrealistic Hope       0.00      0.00      0.00        60\n",
      "\n",
      "        accuracy                           0.34       200\n",
      "       macro avg       0.23      0.32      0.25       200\n",
      "    weighted avg       0.24      0.34      0.26       200\n",
      "\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Accuracy: 36.00%\n",
      "Macro F1 (PRIMARY): 0.2465\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.36      0.83      0.50        72\n",
      "  Realistic Hope       0.35      0.18      0.24        68\n",
      "Unrealistic Hope       0.00      0.00      0.00        60\n",
      "\n",
      "        accuracy                           0.36       200\n",
      "       macro avg       0.24      0.34      0.25       200\n",
      "    weighted avg       0.25      0.36      0.26       200\n",
      "\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Accuracy: 38.50%\n",
      "Macro F1 (PRIMARY): 0.2901\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.38      0.75      0.50        72\n",
      "  Realistic Hope       0.40      0.34      0.37        68\n",
      "Unrealistic Hope       0.00      0.00      0.00        60\n",
      "\n",
      "        accuracy                           0.39       200\n",
      "       macro avg       0.26      0.36      0.29       200\n",
      "    weighted avg       0.27      0.39      0.31       200\n",
      "\n",
      "\n",
      "======================\n",
      "MODEL: Kimi-K2\n",
      "======================\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Accuracy: 51.00%\n",
      "Macro F1 (PRIMARY): 0.5038\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.45      0.69      0.54        72\n",
      "  Realistic Hope       0.54      0.40      0.46        68\n",
      "Unrealistic Hope       0.66      0.42      0.51        60\n",
      "\n",
      "        accuracy                           0.51       200\n",
      "       macro avg       0.55      0.50      0.50       200\n",
      "    weighted avg       0.54      0.51      0.50       200\n",
      "\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Accuracy: 50.00%\n",
      "Macro F1 (PRIMARY): 0.4916\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.44      0.69      0.54        72\n",
      "  Realistic Hope       0.53      0.38      0.44        68\n",
      "Unrealistic Hope       0.63      0.40      0.49        60\n",
      "\n",
      "        accuracy                           0.50       200\n",
      "       macro avg       0.53      0.49      0.49       200\n",
      "    weighted avg       0.53      0.50      0.49       200\n",
      "\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Accuracy: 49.00%\n",
      "Macro F1 (PRIMARY): 0.4877\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.44      0.62      0.51        72\n",
      "  Realistic Hope       0.49      0.40      0.44        68\n",
      "Unrealistic Hope       0.62      0.43      0.51        60\n",
      "\n",
      "        accuracy                           0.49       200\n",
      "       macro avg       0.52      0.49      0.49       200\n",
      "    weighted avg       0.51      0.49      0.49       200\n",
      "\n",
      "\n",
      "======================\n",
      "MODEL: Qwen-32B\n",
      "======================\n",
      "\n",
      "--- Prompt 1 ---\n",
      "Accuracy: 36.50%\n",
      "Macro F1 (PRIMARY): 0.2329\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.37      0.92      0.52        72\n",
      "  Realistic Hope       0.00      0.00      0.00        68\n",
      "Unrealistic Hope       0.35      0.12      0.17        60\n",
      "\n",
      "        accuracy                           0.36       200\n",
      "       macro avg       0.24      0.34      0.23       200\n",
      "    weighted avg       0.24      0.36      0.24       200\n",
      "\n",
      "\n",
      "--- Prompt 2 ---\n",
      "Accuracy: 36.50%\n",
      "Macro F1 (PRIMARY): 0.2329\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.37      0.92      0.52        72\n",
      "  Realistic Hope       0.00      0.00      0.00        68\n",
      "Unrealistic Hope       0.35      0.12      0.17        60\n",
      "\n",
      "        accuracy                           0.36       200\n",
      "       macro avg       0.24      0.34      0.23       200\n",
      "    weighted avg       0.24      0.36      0.24       200\n",
      "\n",
      "\n",
      "--- Prompt 3 ---\n",
      "Accuracy: 36.50%\n",
      "Macro F1 (PRIMARY): 0.2329\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "Generalized Hope       0.37      0.92      0.52        72\n",
      "  Realistic Hope       0.00      0.00      0.00        68\n",
      "Unrealistic Hope       0.35      0.12      0.17        60\n",
      "\n",
      "        accuracy                           0.36       200\n",
      "       macro avg       0.24      0.34      0.23       200\n",
      "    weighted avg       0.24      0.36      0.24       200\n",
      "\n",
      "\n",
      "‚úÖ Saved: Mtemp.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"===== üé® MULTICLASS CLASSIFICATION (ALL MODELS) =====\")\n",
    "\n",
    "rows = []\n",
    "\n",
    "# ============================\n",
    "# Etiquetas finales (INGL√âS)\n",
    "# ============================\n",
    "VALID_LABELS = [\n",
    "    \"Generalized Hope\",\n",
    "    \"Realistic Hope\",\n",
    "    \"Unrealistic Hope\"\n",
    "]\n",
    "\n",
    "# ============================\n",
    "# Normalizaci√≥n ROBUSTA\n",
    "# (Espa√±ol ‚Üí Ingl√©s + Ingl√©s)\n",
    "# ============================\n",
    "def normalize_multiclass(label):\n",
    "    if not isinstance(label, str):\n",
    "        return \"Generalized Hope\"\n",
    "\n",
    "    t = label.strip().lower()\n",
    "\n",
    "    # ----- Espa√±ol -----\n",
    "    if \"deseo\" in t or \"fant√°stico\" in t or \"fantastico\" in t:\n",
    "        return \"Unrealistic Hope\"\n",
    "    if \"realista\" in t:\n",
    "        return \"Realistic Hope\"\n",
    "    if \"abstracta\" in t or \"abstracto\" in t:\n",
    "        return \"Generalized Hope\"\n",
    "\n",
    "    # ----- Ingl√©s -----\n",
    "    if \"fantasy\" in t or \"unrealistic\" in t:\n",
    "        return \"Unrealistic Hope\"\n",
    "    if \"realistic\" in t:\n",
    "        return \"Realistic Hope\"\n",
    "    if \"abstract\" in t or \"generalized\" in t:\n",
    "        return \"Generalized Hope\"\n",
    "\n",
    "    return \"Generalized Hope\"\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Guardar prompts usados\n",
    "# ============================\n",
    "os.makedirs(\"used_prompts\", exist_ok=True)\n",
    "\n",
    "# ============================\n",
    "# LOOP POR MODELO Y PROMPT\n",
    "# ============================\n",
    "for model_name in MODELS.keys():\n",
    "    print(f\"\\n======================\")\n",
    "    print(f\"MODEL: {model_name}\")\n",
    "    print(\"======================\")\n",
    "\n",
    "    for p in [1, 2, 3]:\n",
    "        col = f\"{model_name}_p{p}\"\n",
    "\n",
    "        if col not in finMulti.columns:\n",
    "            print(f\"‚è≠Ô∏è Missing {col}, skipping\")\n",
    "            continue\n",
    "\n",
    "        # Normalizar predicciones\n",
    "        finMulti[f\"{col}_norm\"] = finMulti[col].apply(normalize_multiclass)\n",
    "\n",
    "        # Ground truth en INGL√âS\n",
    "        y_true = finMulti[\"multiclass\"]      # Generalized / Realistic / Unrealistic Hope\n",
    "        y_pred = finMulti[f\"{col}_norm\"]\n",
    "\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        report = classification_report(\n",
    "            y_true,\n",
    "            y_pred,\n",
    "            output_dict=True,\n",
    "            zero_division=0\n",
    "        )\n",
    "\n",
    "        macro_f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "\n",
    "        # Conteo de outputs inv√°lidos (antes de normalizar)\n",
    "        invalid_outputs = sum(\n",
    "            1 for x in finMulti[col]\n",
    "            if not isinstance(x, str)\n",
    "            or (\n",
    "                all(\n",
    "                    kw not in x.lower()\n",
    "                    for kw in [\n",
    "                        \"abstract\", \"general\",\n",
    "                        \"realistic\", \"realista\",\n",
    "                        \"fantasy\", \"fant√°stico\", \"fantastico\",\n",
    "                        \"unrealistic\", \"deseo\"\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(f\"\\n--- Prompt {p} ---\")\n",
    "        print(f\"Accuracy: {acc*100:.2f}%\")\n",
    "        print(f\"Macro F1 (PRIMARY): {macro_f1:.4f}\")\n",
    "        print(classification_report(y_true, y_pred, zero_division=0))\n",
    "\n",
    "        # ============================\n",
    "        # M√©tricas por clase\n",
    "        # ============================\n",
    "        for label in VALID_LABELS:\n",
    "            rows.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Prompt\": p,\n",
    "                \"PromptType\": [\"zero-shot\", \"one-shot\", \"few-shot\"][p-1],\n",
    "                \"Label\": label,\n",
    "                \"Precision\": report[label][\"precision\"],\n",
    "                \"Recall\": report[label][\"recall\"],\n",
    "                \"F1\": report[label][\"f1-score\"],\n",
    "                \"Support\": report[label][\"support\"],\n",
    "                \"Accuracy\": acc,\n",
    "                \"Primary_Macro_F1\": macro_f1,\n",
    "                \"InvalidOutputs\": invalid_outputs\n",
    "            })\n",
    "\n",
    "        # Macro avg\n",
    "        rows.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Prompt\": p,\n",
    "            \"PromptType\": [\"zero-shot\", \"one-shot\", \"few-shot\"][p-1],\n",
    "            \"Label\": \"macro avg\",\n",
    "            \"Precision\": report[\"macro avg\"][\"precision\"],\n",
    "            \"Recall\": report[\"macro avg\"][\"recall\"],\n",
    "            \"F1\": macro_f1,\n",
    "            \"Support\": report[\"macro avg\"][\"support\"],\n",
    "            \"Accuracy\": acc,\n",
    "            \"Primary_Macro_F1\": macro_f1,\n",
    "            \"InvalidOutputs\": invalid_outputs\n",
    "        })\n",
    "\n",
    "        # Weighted avg\n",
    "        rows.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Prompt\": p,\n",
    "            \"PromptType\": [\"zero-shot\", \"one-shot\", \"few-shot\"][p-1],\n",
    "            \"Label\": \"weighted avg\",\n",
    "            \"Precision\": report[\"weighted avg\"][\"precision\"],\n",
    "            \"Recall\": report[\"weighted avg\"][\"recall\"],\n",
    "            \"F1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "            \"Support\": report[\"weighted avg\"][\"support\"],\n",
    "            \"Accuracy\": acc,\n",
    "            \"Primary_Macro_F1\": macro_f1,\n",
    "            \"InvalidOutputs\": invalid_outputs\n",
    "        })\n",
    "\n",
    "        # Guardar prompt usado (referencia metodol√≥gica)\n",
    "        with open(f\"used_prompts/multiclass_{model_name}_prompt{p}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"MODEL: {model_name}\\nPROMPT VERSION: {p}\\n\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Guardar CSV final\n",
    "# =========================\n",
    "df_multi_metrics = pd.DataFrame(rows)\n",
    "df_multi_metrics.to_csv(\n",
    "    \"Mtemp.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Saved: Mtemp.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "358cc40c-a514-4a99-849b-b7982ac50b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Theas\\.virtualenvs\\Ciencia_de_datos-jkqBnT1m\\Lib\\site-packages\\sklearn\\metrics\\_plot\\confusion_matrix.py:143: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  fig, ax = plt.subplots()\n",
      "C:\\Users\\Theas\\AppData\\Local\\Temp\\ipykernel_34948\\483900793.py:25: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(6, 6))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.makedirs(\"confusion_matrices\", exist_ok=True)\n",
    "\n",
    "for model_name in MODELS.keys():\n",
    "    for p in [1, 2, 3]:\n",
    "        col = f\"{model_name}_p{p}\"\n",
    "        if col not in finMulti.columns:\n",
    "            continue\n",
    "\n",
    "        y_true = finMulti[\"multiclass\"]\n",
    "        y_pred = finMulti[col].apply(normalize_multiclass)\n",
    "\n",
    "        labels = [\n",
    "            \"Generalized Hope\",\n",
    "            \"Realistic Hope\",\n",
    "            \"Unrealistic Hope\"\n",
    "        ]\n",
    "\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "        disp = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
    "        plt.title(f\"{model_name} ‚Äî Prompt {p}\")\n",
    "        plt.savefig(f\"confusion_matrices/{model_name}_prompt{p}.png\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd16c31-8928-4c22-8a09-8ed3239d2f99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
